{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming with Python\n",
    "\n",
    "Python has the ability to run in parallel, using both shared memory and distributed memory methods.  This tutorial is meant to give you a brief introduction to what's available and, more importantly, when it's appropriate to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Threading\n",
    "\n",
    "Generally, Python threading is terrible.  But it shouldn't be:\n",
    "\n",
    "* POSIX threads\n",
    "* Shared memory with parent process\n",
    "* Lightweight threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Interpreter Lock (GIL)\n",
    "\n",
    "In order to keep memory coherent, the Python intrepter only allows a single thread to run at once....killing performance for any kind of shared memory workload.\n",
    "\n",
    "There are (some) good reasons for this (I/O, intrepreter maintenance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Calculate Pi with Python Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple process:\n",
    "* Inscribe a circle in a square\n",
    "* Throw darts at it\n",
    "* Count how many are inside the circle and how many are outside\n",
    "* Use the ratio of those to compute pi\n",
    "\n",
    "<img src=\"../img/circle_and_square.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.138488\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "import random\n",
    "\n",
    "lock = Lock() # lock for making operations atomic\n",
    "\n",
    "def calcInside(nsamples,rank):\n",
    "    global inside # we need something everyone can share random.seed(rank)\n",
    "    random.seed(rank)\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x*x)+(y*y)<1:\n",
    "            lock.acquire() # GIL doesn't always save you\n",
    "            inside += 1\n",
    "            lock.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nt=4 # thread count\n",
    "    inside = 0 # initialise\n",
    "    samples=int(10e5/nt)\n",
    "    threads=[Thread(target=calcInside, args=(samples,i)) for i in range(nt)]\n",
    "    \n",
    "    for t in threads: t.start()\n",
    "    for t in threads: t.join()\n",
    "    \n",
    "    print((4.0*inside)/(1.0*samples*nt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above code:\n",
    "\n",
    "* Python data-structures (e.g. lists, dictionaries, etc.) are thread-safe, meaning that when updating their values the GIL is held and other threads wait until it's released before updating object  \n",
    "<br>\n",
    "* Simpler data structures like integegers aren't thread-safe, which means we could have mulitple threads accessing the same object and either data could be corrupted or missed  \n",
    "<br>\n",
    "* We have to explicitly lock our `inside` counter in the above code to avoid this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprocess\n",
    "\n",
    "Python's `subprocess` module allows the Python intrepter to spawn and control processes that aren't affected by the GIL.  The basic command in the `subprocess` module is `Popen()`, which lets you open a proces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "pi=subprocess.Popen('python -c \"import math; print(math.pi)\"',shell=True,stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'3.141592653589793\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues with subprocess:\n",
    "* Shared memory is tricky at best\n",
    "* Locks and atomics are difficult\n",
    "\n",
    "It's really designed for launching independent processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmds = [\n",
    "    'echo foo',\n",
    "    'echo bar',\n",
    "    'date',\n",
    "    'hostname'\n",
    "]\n",
    "\n",
    "tasks = [subprocess.Popen(c, shell=True) for c in cmds]\n",
    "for t in tasks: t.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the output of those subprocesses with `docker logs jupyter`\n",
    "\n",
    "```console\n",
    "foo\n",
    "bar\n",
    "Wed Apr 10 05:48:08 UTC 2019\n",
    "023c8899762c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another method..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "This module blends together Python threads and subprocesses.  It bypasses the GIL, so threads can be used and see some performance.  Under the hood it uses subprocesses, but has a manager to handle things like synchronization and distribuited sharing (but still not true shared memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating pi with Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Pool` module to calculate pi.  `Pool` allows you to define a group of worker processes that you will then divide some work amongst.  `Pool` takes two inputs:\n",
    "\n",
    "* A function that we want to run across the pool of workers\n",
    "* An iterable...some way to identify how we're splitting up work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14226\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "processes = mp.cpu_count()\n",
    "nsamples = int(10e5/processes)\n",
    "\n",
    "def calcInside(rank):\n",
    "    inside = 0\n",
    "    random.seed(rank)\n",
    "    for i in range(nsamples):\n",
    "        x = random.random();\n",
    "        y = random.random();\n",
    "        if (x*x)+(y*y)<1:\n",
    "            inside += 1\n",
    "    return (4.0*inside)/nsamples\n",
    "\n",
    "# Important to check if main so child processes don't try to run it\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(processes)\n",
    "    result = pool.map(calcInside, range(processes))\n",
    "    print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Multiprocessing` module has support for other parallel constructs like process communication and locks.  We won't go into them today, but you should be aware of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Multiprocessing is certainly an improvement over `subprocess` and Python threads, it does come with overhead that impacts performance.  Additionally, it will only scale to a single node (no distributed memory capability).\n",
    "\n",
    "In order to do that, we need..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mpi4py` is a set of bindings to make use of MPI, Message Passing Interface.  MPI forms the basis of most applications that run on HPC systems today.  We won't cover MPI today, but it is important to understand a few basics to understand what `mpi4py` is doing.\n",
    "\n",
    "Simply, all MPI allows is for processors to communicate data between each other.   Each process executes the same instructions (or code), but on different parts of the data.  At points throughout the computation, they may need to send or receive data to/from memory locations that are non-local.  MPI is the API that allows for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "size = MPI.COMM_WORLD.Get_size()\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "\n",
    "sys.stdout.write(\n",
    "    \"Hello, World! I am process %d of %d on %s.\\n\"\n",
    "    % (rank, size, name))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! I am process 0 of 4 on 023c8899762c.\r\n",
      "Hello, World! I am process 2 of 4 on 023c8899762c.\r\n",
      "Hello, World! I am process 3 of 4 on 023c8899762c.\r\n",
      "Hello, World! I am process 1 of 4 on 023c8899762c.\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/helloworld.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-Point Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print('On process 1, data is ',data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On process 1, data is  {'a': 7, 'b': 3.14}\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/pt2pt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sent a dictionary, but we can also send NumPy arrays (and we should try to do that all the time):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # in real code, this section might\n",
    "    # read in data parameters from a file\n",
    "    numData = 10  \n",
    "    comm.send(numData, dest=1)\n",
    "\n",
    "    data = np.linspace(0.0,3.14,numData)  \n",
    "    comm.Send(data, dest=1)\n",
    "\n",
    "elif rank == 1:\n",
    "\n",
    "    numData = comm.recv(source=0)\n",
    "    print('Number of data to receive: ',numData)\n",
    "\n",
    "    data = np.empty(numData, dtype='d')  # allocate space to receive the array\n",
    "    comm.Recv(data, source=0)\n",
    "\n",
    "    print('data received: ',data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data to receive:  10\r\n",
      "data received:  [0.         0.34888889 0.69777778 1.04666667 1.39555556 1.74444444\r\n",
      " 2.09333333 2.44222222 2.79111111 3.14      ]\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/pt2pt_numpy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collectives are operations that all processors execute together.  They may execute at slightly different times, but they all will call the same function.  These are useful for operations like gathering data onto a root process, or distributing data from one to all.\n",
    "\n",
    "Hers' an example of performing a `gather` operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/gather.png\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()   \n",
    "\n",
    "numDataPerRank = 10  \n",
    "sendbuf = np.linspace(rank*numDataPerRank+1,(rank+1)*numDataPerRank,numDataPerRank)\n",
    "print('Rank: ',rank, ', sendbuf: ',sendbuf)\n",
    "\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty(numDataPerRank*size, dtype='d')  \n",
    "\n",
    "comm.Gather(sendbuf, recvbuf, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print('Rank: ',rank, ', recvbuf received: ',recvbuf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank:  3 , sendbuf:  [31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\r\n",
      "Rank:  2 , sendbuf:  [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]\r\n",
      "Rank:  1 , sendbuf:  [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\r\n",
      "Rank:  0 , sendbuf:  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\r\n",
      "Rank:  0 , recvbuf received:  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\r\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\r\n",
      " 37. 38. 39. 40.]\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpi4py has the ability to ship *any* serialisable Python object.  That means that objects like `dicts` need to be converted to a byte stream, a process called pickling.  That means a Python object (except for strings and ints) needs to be pickled, sent over MPI, and then repickled...adding significant overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, arrays in NumPy map to C memory allocations, and mpi4py can send them at *almost* the speed of C/C++/Fortran:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/allgather_bench.png\" style=\"height:450px\">\n",
    "<img src=\"../img/latency_bench.png\" style=\"height:450px\">\n",
    "<img src=\"../img/bandwidth_bench.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pi\n",
    "\n",
    "Now let's look at how we can compute pi with mpi4py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is approximately 3.1450000000000000, error is 0.0034073464102069\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "# Function to calcualte pi that each MPI rank will use\n",
    "def compute_pi(samples):\n",
    "    count = 0\n",
    "    for x, y in samples:\n",
    "        if x**2 + y**2 <= 1:\n",
    "            count += 1\n",
    "    pi = 4*float(count)/len(samples)\n",
    "    return pi\n",
    "\n",
    "# Set up our MPI environment\n",
    "comm = MPI.COMM_WORLD\n",
    "nprocs = comm.Get_size()\n",
    "myrank = comm.Get_rank()\n",
    "\n",
    "# Processor 0 generates random samples that each processor will use\n",
    "if myrank == 0:\n",
    "    N = 100000 // nprocs\n",
    "    samples = numpy.random.random((nprocs, N, 2))\n",
    "else:\n",
    "    samples = None\n",
    "\n",
    "# Distribute the samples amongst all processors wiht MPI_Scatter\n",
    "samples = comm.scatter(samples, root=0)\n",
    "\n",
    "# Each processors calculates their value of pi (we'll take the average)\n",
    "mypi = compute_pi(samples) / nprocs\n",
    "\n",
    "# MPI_Reduce collects all individual \n",
    "pi = comm.reduce(mypi, op=MPI.SUM, root=0)\n",
    "\n",
    "if myrank == 0:\n",
    "    error = abs(pi - numpy.pi)\n",
    "    print(\"pi is approximately %.16f, error is %.16f\" % (pi, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That runs on a single MPI process, so let's launch it in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is approximately 3.1414663999999997, error is 0.0001262535897935\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 ../code/mpi4py/pi_mpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ipyparallel\n",
    "\n",
    "ipyparallel is Python package for creating and running clusters in Jupyter (used to be known as IPython.parallel).  It offers a nice, interactive method for developing and running parallel Python applications.\n",
    "\n",
    "I'd still recommend using a more traditional approach of a standalone Python script and batch script submitted to a scheduler for large-scale production runs, but for rapid development and prototyping, ipyparalell is a valuable tool.\n",
    "\n",
    "There a 4 parts to the ipyparallel architecture:\n",
    "\n",
    "<img src=\"../img/ipyparallel_overview.png\" style=\"height:450px\">\n",
    "\n",
    "**Engine**  \n",
    "An engine is a Python instance that can accept commands, run code, and return results.  You can run multiple engines, allowing for parallel and distributed computing.\n",
    "\n",
    "**Schedulers**  \n",
    "Any commands that are to be run on an engine first go through a scheduler.  The engines will block when executing code, so the scheduler will manage requests in the background.\n",
    "\n",
    "**Client**  \n",
    "This ia a Python object that lets you connect to a ipyparallel cluster.  \n",
    "**Hub**  \n",
    "The hub is the brain of an ipyparallel cluster.  It manages connections to engines, shcedulers, and clients\n",
    "\n",
    "We won't go into all the details of ipyparallel (there's a lot), but we will start a small cluster to run our mpi4py pi code.\n",
    "\n",
    "You can start clusters via command line options:\n",
    "\n",
    "```python\n",
    "ipcluster start -n 4\n",
    "```\n",
    "but we'll do it via a Jupyter notebook extension.  There is an `Ipython Clusters` tab where you can start a cluster.  Select 4 engines and start the cluster:\n",
    "\n",
    "<img src=\"../img/jupyter-cluster.png\" style=\"height:250px\">\n",
    "\n",
    "It will take a few seconds to start up the cluster, but once it's running we can create a client that we'll use to connect to our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "client = ipp.Client()\n",
    "client.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the IDs of the 4 engines we have runninng in our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ipyparallel has a concept called a **view**, which is way to access the engines availalble.  A **direct** view simply lets you send explicit commands to specific engines (e.g. tell each engine to run the same `compute_pi()` function).  A **load balanced** view is more like the `multiprocessing` module we saw previously; you send a command to a pool of workers, and the scheduler will handle which engine it runs on (depending on which one is available).\n",
    "\n",
    "We're going to just focus on the direct views in this example.  To start we simply define what engines we want to use in our direct view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DirectView [0, 1, 2, 3]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview = client[:]\n",
    "dview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the same pi calculation code we've seen before, but we're going to do in parallel without mpi4py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from math import pi\n",
    "dview['random'] = random\n",
    "\n",
    "# Serial version\n",
    "def serial_pi(nsamples):\n",
    "    s = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random()\n",
    "        y = random()\n",
    "        if x*x + y*y <= 1:\n",
    "            s+=1\n",
    "    return 4.*s/nsamples\n",
    "\n",
    "# Parallel version\n",
    "def parallel_pi(view, nsamples):\n",
    "    p = len(view.targets)\n",
    "    if nsamples % p:\n",
    "        # ensure even divisibility\n",
    "        nsamples += p - (nsamples%p)\n",
    "    \n",
    "    subsamples = nsamples//p\n",
    "    \n",
    "    ar = view.apply(serial_pi, subsamples)\n",
    "    return sum(ar)/p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is similar to the `mpi4py` version:\n",
    "* We define a serial algorithm\n",
    "* We divide up the number of samples based on the number of engines in our view\n",
    "* The serial algorithm is applied across all engines, and the final answer is aggreated.\n",
    "\n",
    "Let's see the serial version performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 s ± 84.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "serial_pi(int(1e7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the parallel version, passing in the list of engines in our view that we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41 s ± 17.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "parallel_pi(dview, int(1e7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client MPI.COMM_WORLD.Get_size()=1\n",
      "Client engine ids [0, 1, 2, 3]\n",
      "pi is approximately 3.1496400000000002, error is 0.0080473464102071\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "# Function to calcualte pi that each MPI rank will use\n",
    "def compute_pi(samples):\n",
    "    count = 0\n",
    "    for x, y in samples:\n",
    "        if x**2 + y**2 <= 1:\n",
    "            count += 1\n",
    "    pi = 4*float(count)/len(samples)\n",
    "    return pi\n",
    "\n",
    "client = ipp.Client(profile='mpi')\n",
    "view = client[:]\n",
    "\n",
    "print(\"Client MPI.COMM_WORLD.Get_size()=%s\" % MPI.COMM_WORLD.Get_size())\n",
    "print(\"Client engine ids %s\" % client.ids)\n",
    "\n",
    "# Set up our MPI environment\n",
    "comm = MPI.COMM_WORLD\n",
    "nprocs = comm.Get_size()\n",
    "myrank = comm.Get_rank()\n",
    "\n",
    "# Processor 0 generates random samples that each processor will use\n",
    "if myrank == 0:\n",
    "    N = 100000 // nprocs\n",
    "    samples = numpy.random.random((nprocs, N, 2))\n",
    "else:\n",
    "    samples = None\n",
    "# Distribute the samples amongst all processors wiht MPI_Scatter\n",
    "samples = comm.scatter(samples, root=0)\n",
    "# Each processors calculates their value of pi (we'll take the average)\n",
    "mypi = compute_pi(samples) / nprocs\n",
    "# MPI_Reduce collects all individual \n",
    "pi = comm.reduce(mypi, op=MPI.SUM, root=0)\n",
    "if myrank == 0:\n",
    "    error = abs(pi - numpy.pi)\n",
    "    print(\"pi is approximately %.16f, error is %.16f\" % (pi, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example\n",
    "\n",
    "For those that don't consider Python a vialble HPC language, here's an example of what you can do with Python at scale:\n",
    "\n",
    "<img src=\"../img/pyfr_logo.png\" style=\"height:250px\">\n",
    "\n",
    "\n",
    "[PyFR](http://www.pyfr.org/index.php) - A Python framework for solving advection-diffucions problems.\n",
    "\n",
    "Features:\n",
    "* **Multi-platform**\n",
    "    * AMD GPUs\n",
    "    * NVIDIA GPUs\n",
    "    * CPUs\n",
    "    * Intel MIC\n",
    "    * Even Raspberyy Pi  \n",
    "<br>\n",
    "* **Parallelism**\n",
    "    * MPI (mpi4py)\n",
    "    * CUDA (PyCUDA)\n",
    "    * OpenMP (pyMIC for KNL)\n",
    "    * OpenCL (PyOpenCL)\n",
    "    * HDF5 Parallel I/O (h5py)  \n",
    "<br>    \n",
    "* **Scalable**\n",
    "    * 18,000 K20X GPUs on Titan (ORNL)\n",
    "    * 195 billion DOFs\n",
    "    * 58% peak performance (Summit HPL benchmark was 71%)\n",
    "    * SC16 Best Paper/Gordon Bell nominee\n",
    "    \n",
    "And it does all that in about **8000 lines of code**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/pyfr-sim.gif\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler Demo (from PyFR website)\n",
    "\n",
    "Here we'll run a small PyFR demo, a 2D Euler vortex simulation.  We have a separate Conda environment in our notebook for PyFR (select the pyfr kernel from the Kernel menu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/conda-nb.png\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** For this small example, runnning in parallel inside our notebook give pretty terrible performance.  I've written the commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyfr import ../code/pyfr/euler_vortex_2d.msh ../code/pyfr/euler_vortex_2d.pyfrm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to partition the mesh depending on how many processors we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\r\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\r\n"
     ]
    }
   ],
   "source": [
    "!pyfr partition 4 ../code/pyfr/euler_vortex_2d.pyfrm ../code/pyfr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/shapes.py:274: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "\u001b[2K\u001b[G   0.0% [>                           ] 0.01/100.00 ela: 00:00:02 rem: 14:47:22\u001b[2K\u001b[G   0.0% [>                           ] 0.01/100.00 ela: 00:00:04 rem: 13:32:03\u001b[2K\u001b[G   0.0% [>                           ] 0.01/100.00 ela: 00:00:07 rem: 13:09:03\u001b[2K\u001b[G   0.0% [>                           ] 0.02/100.00 ela: 00:00:09 rem: 13:00:47\u001b[2K\u001b[G   0.0% [>                           ] 0.03/100.00 ela: 00:00:11 rem: 13:04:37\u001b[2K\u001b[G   0.0% [>                           ] 0.03/100.00 ela: 00:00:13 rem: 12:49:22^C\n",
      "[mpiexec@023c8899762c] Sending Ctrl-C to processes as requested\n",
      "[mpiexec@023c8899762c] Press Ctrl-C again to force abort\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/shapes.py:274: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/shapes.py:274: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/shapes.py:274: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pyfr/bin/pyfr\", line 11, in <module>\n",
      "    load_entry_point('pyfr==1.8.0', 'console_scripts', 'pyfr')()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 110, in main\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 235, in process_run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 227, in _process_common\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/base.py\", line 98, in run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/controllers.py\", line 70, in advance_to\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/steppers.py\", line 140, in step\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/solvers/baseadvec/system.py\", line 42, in rhs\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/backend.py\", line 176, in runall\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/types.py\", line 104, in runall\n",
      "    if kwds is None:\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 326, in _exec_nowait\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 311, in _exec_item\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/gimmik.py\", line 38, in run\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pyfr/bin/pyfr\", line 11, in <module>\n",
      "    load_entry_point('pyfr==1.8.0', 'console_scripts', 'pyfr')()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 110, in main\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 235, in process_run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 227, in _process_common\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/base.py\", line 98, in run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/controllers.py\", line 70, in advance_to\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/steppers.py\", line 140, in step\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/solvers/baseadvec/system.py\", line 42, in rhs\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/backend.py\", line 176, in runall\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/types.py\", line 104, in runall\n",
      "    if kwds is None:\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 326, in _exec_nowait\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 311, in _exec_item\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/gimmik.py\", line 38, in run\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pyfr/bin/pyfr\", line 11, in <module>\n",
      "    load_entry_point('pyfr==1.8.0', 'console_scripts', 'pyfr')()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 110, in main\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 235, in process_run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 227, in _process_common\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/base.py\", line 98, in run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/controllers.py\", line 70, in advance_to\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/steppers.py\", line 140, in step\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/solvers/baseadvec/system.py\", line 42, in rhs\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/backend.py\", line 176, in runall\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/types.py\", line 104, in runall\n",
      "    if kwds is None:\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 326, in _exec_nowait\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 311, in _exec_item\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/provider.py\", line 24, in run\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pyfr/bin/pyfr\", line 11, in <module>\n",
      "    load_entry_point('pyfr==1.8.0', 'console_scripts', 'pyfr')()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 110, in main\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 235, in process_run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 227, in _process_common\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/base.py\", line 98, in run\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/controllers.py\", line 70, in advance_to\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/integrators/std/steppers.py\", line 140, in step\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/solvers/baseadvec/system.py\", line 42, in rhs\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/backend.py\", line 176, in runall\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/types.py\", line 104, in runall\n",
      "    if kwds is None:\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 326, in _exec_nowait\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/base/types.py\", line 311, in _exec_item\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/backends/openmp/gimmik.py\", line 38, in run\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 pyfr run -b openmp -p ../code/pyfr/euler_vortex_2d.pyfrm ../code/pyfr/euler_vortex_2d.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyFR outputs in VTK format, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "view = c[:]\n",
    "view.activate()\n",
    "view.run('/opt/conda/envs/pyfr/bin run -b openmp -p ../code/pyfr/euler_vortex_2d.pyfrm ../code/pyfr/euler_vortex_2d.ini')\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: execute>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = c[:]\n",
    "view.activate()\n",
    "view.run('/opt/conda/envs/pyfr/bin/pyfr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba\n",
    "\n",
    "Numba provides code compilation like Cython, but does so in a simple, easy-to-use fashion.  All that's needed is to add a decorator (similar to a C/C++ pragma or Fortran directive) to compute kernel\n",
    "\n",
    "```python\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def func(x):\n",
    "    # some loop or comutationally intensive kernel\n",
    "    return x\n",
    "```\n",
    "\n",
    "Python code with the `@jit` decorator are compiled at runtime (just-in-time) using the LLVM compiler, producing code that is on par with C/C++ and Fortran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it look like:\n",
    "<img src=\"../img/numba_process.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python at Pawsey\n",
    "\n",
    "Pawsey has a number of solutions for Python users:\n",
    "\n",
    "- Compiled Python modules (Versions 2&3)\n",
    "- Tuned NumPy/SciPy libraries (linked agains MKL and Cray-LibSci)\n",
    "- Job-Packing Methods\n",
    "- Shifter/Singularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job-Packing\n",
    "\n",
    "Users of Magnus and Galaxy are allocated an entire node, and charged accordingly, whether they use it all or not.  Many users want to run as many single-core Python jobs on a node as possible.  The easiest way to do that is to use job-packing in your SLURM jobscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash -l\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --ntasks-per-node=24\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --partition=debugq\n",
    "#SBATCH --account=pawsey0001\n",
    "#SBATCH --export=NONE\n",
    "\n",
    "module swap PrgEnv-crady PrgEnv-gnu\n",
    "module load python\n",
    "module load numpy\n",
    "module load scipy\n",
    "module load matplotlib\n",
    "\n",
    "srun --export=ALL -n 24 -N 1 python_job_wrapper.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a single wrapper script across 24 cores.  The key is how we write our wrapper script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "python voxelSlice.py qs-curie-${SLURM_PROCID}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance of the wrapper script will call the Python interpreter, but we use the environment variable `SLURM_PROCID` to differentiate between the cores, and each core takes a different input data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit with this method is it usually require no changes to existing Python scripts, but may require some thought be given as to how to structure data inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pawsey also has Docker images available to use, particularly for Python users.  We have a program called Shifter installed on our Cray systems.  It allows for Docker containers to be run on a shared HPC system, while still maintaining performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Shifter_OSU_allgather.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Shifter_OSU_bandwidth_reduced.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job scripts require minimal modification:\n",
    "    \n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --image=docker:pawsey/hpc-python:latest\n",
    " \n",
    " \n",
    "module load shifter\n",
    " \n",
    " \n",
    "srun -n 24 shifter python my_python_app.py <args>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Docker images provide a base of what most users would need to build their own images:\n",
    "    \n",
    "```docker\n",
    "FROM ubuntu:latest\n",
    "\n",
    "LABEL maintainer=\"brian.skjerven@pawsey.org.au\"\n",
    "\n",
    "RUN apt-get update \\\n",
    "      && apt-get install -y \\\n",
    "      cython \\\n",
    "      python-minimal \\\n",
    "      python-pip\n",
    "\n",
    "RUN pip install --upgrade pip \\\n",
    "      && pip install \\\n",
    "      astropy \\\n",
    "      h5py \\\n",
    "      matplotlib \\\n",
    "      nose \\\n",
    "      numpy \\\n",
    "      pytest \\\n",
    "      scipy \\\n",
    "      setuptools\n",
    "\n",
    "CMD [\"/bin/bash\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other benefit to using Python in a container is related to dynamic library loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/shifter_magnus.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "- Lots of diferent ways of explointing parallelism in Python\n",
    "    * Some better sutied to different workflows\n",
    "- Make use of Pawsey compiled Python libraries (performance and module compatibility)\n",
    "- Try to use MPI capable libraries\n",
    "- Multiprocess *can* be useful, but there is a performance hit\n",
    "- Other Python options available to users (Shifter, job-packing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyfr]",
   "language": "python",
   "name": "conda-env-pyfr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
