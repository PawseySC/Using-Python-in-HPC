{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming with Python\n",
    "\n",
    "Python has the ability to run in parallel, using both shared memory and distributed memory methods.  This tutorial is meant to give you a brief introduction to what's available and, more importantly, when it's appropriate to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Threading\n",
    "\n",
    "Generally, Python threading is terrible.  But it shouldn't be:\n",
    "\n",
    "* POSIX threads\n",
    "* Shared memory with parent process\n",
    "* Lightweight threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Interpreter Lock (GIL)\n",
    "\n",
    "In order to keep memory coherent, the Python intrepter only allows a single thread to run at once....killing performance for any kind of shared memory workload.\n",
    "\n",
    "There are (some) good reasons for this (I/O, intrepreter maintenance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Calculate Pi with Python Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple process:\n",
    "* Inscribe a circle in a square\n",
    "* Throw darts at it\n",
    "* Count how many are inside the circle and how many are outside\n",
    "* Use the ratio of those to compute pi\n",
    "\n",
    "<img src=\"../img/circle_and_square.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.139148\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "import random\n",
    "\n",
    "lock = Lock() # lock for making operations atomic\n",
    "\n",
    "def calcInside(nsamples,rank):\n",
    "    global inside # we need something everyone can share random.seed(rank)\n",
    "    random.seed(rank)\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x*x)+(y*y)<1:\n",
    "            lock.acquire() # GIL doesn't always save you\n",
    "            inside += 1\n",
    "            lock.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nt=4 # thread count\n",
    "    inside = 0 # initialise\n",
    "    samples=int(10e5/nt)\n",
    "    threads=[Thread(target=calcInside, args=(samples,i)) for i in range(nt)]\n",
    "    \n",
    "    for t in threads: t.start()\n",
    "    for t in threads: t.join()\n",
    "    \n",
    "    print((4.0*inside)/(1.0*samples*nt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above code:\n",
    "\n",
    "* Python data-structures (e.g. lists, dictionaries, etc.) are thread-safe, meaning that when updating their values the GIL is held and other threads wait until it's released before updating object  \n",
    "<br>\n",
    "* Simpler data structures like integegers aren't thread-safe, which means we could have mulitple threads accessing the same object and either data could be corrupted or missed  \n",
    "<br>\n",
    "* We have to explicitly lock our `inside` counter in the above code to avoid this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprocess\n",
    "\n",
    "Python's `subprocess` module allows the Python intrepter to spawn and control processes that aren't affected by the GIL.  The basic command in the `subprocess` module is `Popen()`, which lets you open a proces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "pi=subprocess.Popen('python -c \"import math; print(math.pi)\"',shell=True,stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'3.141592653589793\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues with subprocess:\n",
    "* Shared memory is tricky at best\n",
    "* Locks and atomics are difficult\n",
    "\n",
    "It's really designed for launching independent processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmds = [\n",
    "    'echo foo',\n",
    "    'echo bar',\n",
    "    'date',\n",
    "    'hostname'\n",
    "]\n",
    "\n",
    "tasks = [subprocess.Popen(c, shell=True) for c in cmds]\n",
    "for t in tasks: t.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another method..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "This module blends together Python threads and subprocesses.  It bypasses the GIL, so threads can be used and see some performance.  Under the hood it uses subprocesses, but has a manager to handle things like synchronization and distribuited sharing (but still not true shared memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating pi with Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Pool` module to calculate pi.  `Pool` allows you to define a group of worker processes that you will then divide some work amongst.  `Pool` takes two inputs:\n",
    "\n",
    "* A function that we want to run across the pool of workers\n",
    "* An iterable...some way to identify how we're splitting up work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14226\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "processes = mp.cpu_count()\n",
    "nsamples = int(10e5/processes)\n",
    "\n",
    "def calcInside(rank):\n",
    "    inside = 0\n",
    "    random.seed(rank)\n",
    "    for i in range(nsamples):\n",
    "        x = random.random();\n",
    "        y = random.random();\n",
    "        if (x*x)+(y*y)<1:\n",
    "            inside += 1\n",
    "    return (4.0*inside)/nsamples\n",
    "\n",
    "# Important to check if main so child processes don't try to run it\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(processes)\n",
    "    result = pool.map(calcInside, range(processes))\n",
    "    print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Multiprocessing` module has support for other parallel constructs like process communication and locks.  We won't go into them today, but you should be aware of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Multiprocessing is certainly an improvement over `subprocess` and Python threads, it does come with overhead that impacts performance.  Additionally, it will only scale to a single node (no distributed memory capability).\n",
    "\n",
    "In order to do that, we need..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mpi4py` is a set of bindings to make use of MPI, Message Passing Interface.  MPI forms the basis of most applications that run on HPC systems today.  We won't cover MPI today, but it is important to understand a few basics to understand what `mpi4py` is doing.\n",
    "\n",
    "Simply, all MPI allows is for processors to communicate data between each other.   Each process executes the same instructions (or code), but on different parts of the data.  At points throughout the computation, they may need to send or receive data to/from memory locations that are non-local.  MPI is the API that allows for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "size = MPI.COMM_WORLD.Get_size()\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "\n",
    "sys.stdout.write(\n",
    "    \"Hello, World! I am process %d of %d on %s.\\n\"\n",
    "    % (rank, size, name))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! I am process 2 of 4 on 8819eff33fb3.\r\n",
      "Hello, World! I am process 3 of 4 on 8819eff33fb3.\r\n",
      "Hello, World! I am process 0 of 4 on 8819eff33fb3.\r\n",
      "Hello, World! I am process 1 of 4 on 8819eff33fb3.\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/helloworld.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-Point Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print('On process 1, data is ',data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On process 1, data is  {'a': 7, 'b': 3.14}\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/pt2pt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sent a dictionary, but we can also send NumPy arrays (and we should try to do that all the time):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # in real code, this section might\n",
    "    # read in data parameters from a file\n",
    "    numData = 10  \n",
    "    comm.send(numData, dest=1)\n",
    "\n",
    "    data = np.linspace(0.0,3.14,numData)  \n",
    "    comm.Send(data, dest=1)\n",
    "\n",
    "elif rank == 1:\n",
    "\n",
    "    numData = comm.recv(source=0)\n",
    "    print('Number of data to receive: ',numData)\n",
    "\n",
    "    data = np.empty(numData, dtype='d')  # allocate space to receive the array\n",
    "    comm.Recv(data, source=0)\n",
    "\n",
    "    print('data received: ',data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data to receive:  10\r\n",
      "data received:  [0.         0.34888889 0.69777778 1.04666667 1.39555556 1.74444444\r\n",
      " 2.09333333 2.44222222 2.79111111 3.14      ]\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/pt2pt_numpy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collectives are operations that all processors execute together.  They may execute at slightly different times, but they all will call the same function.  These are useful for operations like gathering data onto a root process, or distributing data from one to all.\n",
    "\n",
    "Hers' an example of performing a `gather` operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/gather.png\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()   \n",
    "\n",
    "numDataPerRank = 10  \n",
    "sendbuf = np.linspace(rank*numDataPerRank+1,(rank+1)*numDataPerRank,numDataPerRank)\n",
    "print('Rank: ',rank, ', sendbuf: ',sendbuf)\n",
    "\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty(numDataPerRank*size, dtype='d')  \n",
    "\n",
    "comm.Gather(sendbuf, recvbuf, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print('Rank: ',rank, ', recvbuf received: ',recvbuf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank:  3 , sendbuf:  [31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\n",
      "Rank:  1 , sendbuf:  [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      "Rank:  2 , sendbuf:  [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]\n",
      "Rank:  0 , sendbuf:  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "Rank:  0 , recvbuf received:  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40.]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../code/mpi4py/gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpi4py has the ability to ship *any* serialisable Python object.  That means that objects like `dicts` need to be converted to a byte stream, a process called pickling.  That means a Python object (except for strings and ints) needs to be pickled, sent over MPI, and then repickled...adding significant overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, arrays in NumPy map to C memory allocations, and mpi4py can send them at *almost* the speed of C/C++/Fortran:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/allgather_bench.png\" style=\"height:450px\">\n",
    "<img src=\"../img/latency_bench.png\" style=\"height:450px\">\n",
    "<img src=\"../img/bandwidth_bench.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba\n",
    "\n",
    "Numba provides code compilation like Cython, but does so in a simple, easy-to-use fashion.  All that's needed is to add a decorator (similar to a C/C++ pragma or Fortran directive) to compute kernel\n",
    "\n",
    "```python\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def func(x):\n",
    "    # some loop or comutationally intensive kernel\n",
    "    return x\n",
    "```\n",
    "\n",
    "Python code with the `@jit` decorator are compiled at runtime (just-in-time) using the LLVM compiler, producing code that is on par with C/C++ and Fortran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it look like:\n",
    "<img src=\"../img/numba_process.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python at Pawsey\n",
    "\n",
    "Pawsey has a number of solutions for Python users:\n",
    "\n",
    "- Compiled Python modules (Versions 2&3)\n",
    "- Tuned NumPy/SciPy libraries (linked agains MKL and Cray-LibSci)\n",
    "- Job-Packing Methods\n",
    "- Shifter/Singularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job-Packing\n",
    "\n",
    "Users of Magnus and Galaxy are allocated an entire node, and charged accordingly, whether they use it all or not.  Many users want to run as many single-core Python jobs on a node as possible.  The easiest way to do that is to use job-packing in your SLURM jobscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "#!/bin/bash -l\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --ntasks-per-node=24\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --partition=debugq\n",
    "#SBATCH --account=pawsey0001\n",
    "#SBATCH --export=NONE\n",
    "\n",
    "module swap PrgEnv-crady PrgEnv-gnu\n",
    "module load python\n",
    "module load numpy\n",
    "module load scipy\n",
    "module load matplotlib\n",
    "\n",
    "srun --export=ALL -n 24 -N 1 python_job_wrapper.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a single wrapper script across 24 cores.  The key is how we write our wrapper script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "python voxelSlice.py qs-curie-${SLURM_PROCID}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance of the wrapper script will call the Python interpreter, but we use the environment variable `SLURM_PROCID` to differentiate between the cores, and each core takes a different input data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit with this method is it usually require no changes to existing Python scripts, but may require some thought be given as to how to structure data inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pawsey also has Docker images available to use, particularly for Python users.  We have a program called Shifter installed on our Cray systems.  It allows for Docker containers to be run on a shared HPC system, while still maintaining performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Shifter_OSU_allgather.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Shifter_OSU_bandwidth_reduced.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job scripts require minimal modification:\n",
    "    \n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --image=docker:pawsey/hpc-python:latest\n",
    " \n",
    " \n",
    "module load shifter\n",
    " \n",
    " \n",
    "srun -n 24 shifter python my_python_app.py <args>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Docker images provide a base of what most users would need to build their own images:\n",
    "    \n",
    "```\n",
    "FROM ubuntu:latest\n",
    "\n",
    "LABEL maintainer=\"brian.skjerven@pawsey.org.au\"\n",
    "\n",
    "RUN apt-get update \\\n",
    "      && apt-get install -y \\\n",
    "      cython \\\n",
    "      python-minimal \\\n",
    "      python-pip\n",
    "\n",
    "RUN pip install --upgrade pip \\\n",
    "      && pip install \\\n",
    "      astropy \\\n",
    "      h5py \\\n",
    "      matplotlib \\\n",
    "      nose \\\n",
    "      numpy \\\n",
    "      pytest \\\n",
    "      scipy \\\n",
    "      setuptools\n",
    "\n",
    "CMD [\"/bin/bash\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other benefit to using Python in a container is related to dynamic library loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/shifter_magnus.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "- Make use of Pawsey compiled Python libraries (performance and module compatibility)\n",
    "- Try to use MPI capable libraries\n",
    "- Multiprocess *can* be useful, but there is a performance hit\n",
    "- Other Python options available to users (Shifter, job-packing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
