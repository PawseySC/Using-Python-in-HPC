{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming with Python\n",
    "\n",
    "Python has the ability to run in parallel, using both shared memory and distributed memory methods.  This tutorial is meant to give you a brief introduction to what's available and, more importantly, when it's appropriate to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Threading\n",
    "\n",
    "Generally, Python threading is terrible.  But it shouldn't be:\n",
    "\n",
    "* POSIX threads\n",
    "* Shared memory with parent process\n",
    "* Lightweight threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Interpreter Lock (GIL)\n",
    "\n",
    "In order to keep memory coherent, the Python interpreter only allows a single thread to run at once....killing performance for any kind of shared memory workload.\n",
    "\n",
    "There are (some) good reasons for this (I/O, interpreter maintenance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Calculate Pi with Python Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple process:\n",
    "* Inscribe a circle in a square\n",
    "* Throw darts at it\n",
    "* Count how many are inside the circle and how many are outside\n",
    "* Use the ratio of those to compute pi\n",
    "\n",
    "<img src=\"../img/circle_and_square.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13878\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Lock\n",
    "import random\n",
    "\n",
    "lock = Lock() # lock for making operations atomic\n",
    "\n",
    "def calcInside(nsamples,rank):\n",
    "    global inside # we need something everyone can share random.seed(rank)\n",
    "    random.seed(rank)\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x*x)+(y*y)<1:\n",
    "            lock.acquire() # GIL doesn't always save you\n",
    "            inside += 1\n",
    "            lock.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nt=4 # thread count\n",
    "    inside = 0 # initialise\n",
    "    samples=int(10e5/nt)\n",
    "    threads=[Thread(target=calcInside, args=(samples,i)) for i in range(nt)]\n",
    "    \n",
    "    for t in threads: t.start()\n",
    "    for t in threads: t.join()\n",
    "    \n",
    "    print((4.0*inside)/(1.0*samples*nt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on the above code:\n",
    "\n",
    "* Python data-structures (e.g. lists, dictionaries, etc.) are thread-safe, meaning that when updating their values the GIL is held and other threads wait until it's released before updating object  \n",
    "<br>\n",
    "* Simpler data structures like integers aren't thread-safe, which means we could have multiple threads accessing the same object and either data could be corrupted or missed  \n",
    "<br>\n",
    "* We have to explicitly lock our `inside` counter in the above code to avoid this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subprocess\n",
    "\n",
    "Python's `subprocess` module allows the Python interpreter to spawn and control processes that aren't affected by the GIL.  The basic command in the `subprocess` module is `Popen()`, which lets you open a proces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "pi=subprocess.Popen('python -c \"import math; print(math.pi)\"',shell=True,stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'3.141592653589793\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.stdout.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues with subprocess:\n",
    "* Shared memory is tricky at best\n",
    "* Locks and atomics are difficult\n",
    "\n",
    "It's really designed for launching independent processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmds = [\n",
    "    'echo foo',\n",
    "    'echo bar',\n",
    "    'date',\n",
    "    'hostname'\n",
    "]\n",
    "\n",
    "tasks = [subprocess.Popen(c, shell=True) for c in cmds]\n",
    "for t in tasks: t.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the output of those subprocesses with `docker logs jupyter`\n",
    "\n",
    "```console\n",
    "foo\n",
    "bar\n",
    "Wed Apr 10 05:48:08 UTC 2019\n",
    "023c8899762c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another method..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "This module blends together Python threads and subprocesses.  It bypasses the GIL, so threads can be used and see some performance.  Under the hood it uses subprocesses, but has a manager to handle things like synchronization and distributed sharing (but still not true shared memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating pi with Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Pool` module to calculate pi.  `Pool` allows you to define a group of worker processes that you will then divide some work amongst.  `Pool` takes two inputs:\n",
    "\n",
    "* A function that we want to run across the pool of workers\n",
    "* An iterable...some way to identify how we're splitting up work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "processes = mp.cpu_count()\n",
    "nsamples = int(10e5/processes)\n",
    "\n",
    "def calcInside(rank):\n",
    "    inside = 0\n",
    "    random.seed(rank)\n",
    "    for i in range(nsamples):\n",
    "        x = random.random();\n",
    "        y = random.random();\n",
    "        if (x*x)+(y*y)<1:\n",
    "            inside += 1\n",
    "    return (4.0*inside)/nsamples\n",
    "\n",
    "# Important to check if main so child processes don't try to run it\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(processes)\n",
    "    result = pool.map(calcInside, range(processes))\n",
    "    print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Multiprocessing` module has support for other parallel constructs like process communication and locks.  We won't go into them today, but you should be aware of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Multiprocessing is certainly an improvement over `subprocess` and Python threads, it does come with overhead that impacts performance.  Additionally, it will only scale to a single node (no distributed memory capability).\n",
    "\n",
    "In order to do that, we need..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mpi4py` is a set of bindings to make use of MPI, Message Passing Interface.  MPI forms the basis of most applications that run on HPC systems today.  We won't cover MPI today, but it is important to understand a few basics to understand what `mpi4py` is doing.\n",
    "\n",
    "Simply, all MPI allows is for processors to communicate data between each other.   Each process executes the same instructions (or code), but on different parts of the data.  At points throughout the computation, they may need to send or receive data to/from memory locations that are non-local.  MPI is the API that allows for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "size = MPI.COMM_WORLD.Get_size()\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "\n",
    "sys.stdout.write(\n",
    "    \"Hello, World! I am process %d of %d on %s.\\n\"\n",
    "    % (rank, size, name))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! I am process 0 of 4 on af37f82bb294.\n",
      "Hello, World! I am process 3 of 4 on af37f82bb294.\n",
      "Hello, World! I am process 1 of 4 on af37f82bb294.\n",
      "Hello, World! I am process 2 of 4 on af37f82bb294.\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../demos/mpi4py/helloworld.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-Point Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print('On process 1, data is ',data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On process 1, data is  {'a': 7, 'b': 3.14}\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../demos/mpi4py/pt2pt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sent a dictionary, but we can also send NumPy arrays (and we should try to do that all the time):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # in real code, this section might\n",
    "    # read in data parameters from a file\n",
    "    numData = 10  \n",
    "    comm.send(numData, dest=1)\n",
    "\n",
    "    data = np.linspace(0.0,3.14,numData)  \n",
    "    comm.Send(data, dest=1)\n",
    "\n",
    "elif rank == 1:\n",
    "\n",
    "    numData = comm.recv(source=0)\n",
    "    print('Number of data to receive: ',numData)\n",
    "\n",
    "    data = np.empty(numData, dtype='d')  # allocate space to receive the array\n",
    "    comm.Recv(data, source=0)\n",
    "\n",
    "    print('data received: ',data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data to receive:  10\r\n",
      "data received:  [0.         0.34888889 0.69777778 1.04666667 1.39555556 1.74444444\r\n",
      " 2.09333333 2.44222222 2.79111111 3.14      ]\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../demos/mpi4py/pt2pt_numpy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collectives are operations that all processors execute together.  They may execute at slightly different times, but they all will call the same function.  These are useful for operations like gathering data onto a root process, or distributing data from one to all.\n",
    "\n",
    "Hers' an example of performing a `gather` operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/gather.png\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()   \n",
    "\n",
    "numDataPerRank = 10  \n",
    "sendbuf = np.linspace(rank*numDataPerRank+1,(rank+1)*numDataPerRank,numDataPerRank)\n",
    "print('Rank: ',rank, ', sendbuf: ',sendbuf)\n",
    "\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty(numDataPerRank*size, dtype='d')  \n",
    "\n",
    "comm.Gather(sendbuf, recvbuf, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print('Rank: ',rank, ', recvbuf received: ',recvbuf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank:  3 , sendbuf:  [31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\n",
      "Rank:  2 , sendbuf:  [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]\n",
      "Rank:  1 , sendbuf:  [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      "Rank:  0 , sendbuf:  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "Rank:  0 , recvbuf received:  [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40.]\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 python ../demos/mpi4py/gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpi4py has the ability to ship *any* serialisable Python object.  That means that objects like `dicts` need to be converted to a byte stream, a process called pickling.  That means a Python object (except for strings and ints) needs to be pickled, sent over MPI, and then repickled...adding significant overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, arrays in NumPy map to C memory allocations, and mpi4py can send them at *almost* the speed of C/C++/Fortran:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/allgather_bench.png\" style=\"height:450px\">\n",
    "<img src=\"../img/latency_bench.png\" style=\"height:450px\">\n",
    "<img src=\"../img/bandwidth_bench.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Pi\n",
    "\n",
    "Now let's look at how we can compute pi with mpi4py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is approximately 3.1374399999999998, error is 0.0041526535897933\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "# Function to calcualte pi that each MPI rank will use\n",
    "def compute_pi(samples):\n",
    "    count = 0\n",
    "    for x, y in samples:\n",
    "        if x**2 + y**2 <= 1:\n",
    "            count += 1\n",
    "    pi = 4*float(count)/len(samples)\n",
    "    return pi\n",
    "\n",
    "# Set up our MPI environment\n",
    "comm = MPI.COMM_WORLD\n",
    "nprocs = comm.Get_size()\n",
    "myrank = comm.Get_rank()\n",
    "\n",
    "# Processor 0 generates random samples that each processor will use\n",
    "if myrank == 0:\n",
    "    N = 100000 // nprocs\n",
    "    samples = numpy.random.random((nprocs, N, 2))\n",
    "else:\n",
    "    samples = None\n",
    "\n",
    "# Distribute the samples amongst all processors wiht MPI_Scatter\n",
    "samples = comm.scatter(samples, root=0)\n",
    "\n",
    "# Each processors calculates their value of pi (we'll take the average)\n",
    "mypi = compute_pi(samples) / nprocs\n",
    "\n",
    "# MPI_Reduce collects all individual \n",
    "pi = comm.reduce(mypi, op=MPI.SUM, root=0)\n",
    "\n",
    "if myrank == 0:\n",
    "    error = abs(pi - numpy.pi)\n",
    "    print(\"pi is approximately %.16f, error is %.16f\" % (pi, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That runs on a single MPI process, so let's launch it in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is approximately 3.1415332000000000, error is 0.0000594535897931\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 4 ../demos/mpi4py/pi_mpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ipyparallel\n",
    "\n",
    "ipyparallel is Python package for creating and running clusters in Jupyter (used to be known as IPython.parallel).  It offers a nice, interactive method for developing and running parallel Python applications.\n",
    "\n",
    "I'd still recommend using a more traditional approach of a standalone Python script and batch script submitted to a scheduler for large-scale production runs, but for rapid development and prototyping, ipyparalell is a valuable tool.\n",
    "\n",
    "There a 4 parts to the ipyparallel architecture:\n",
    "\n",
    "<img src=\"../img/ipyparallel_overview.png\" style=\"height:450px\">\n",
    "\n",
    "**Engine**  \n",
    "An engine is a Python instance that can accept commands, run code, and return results.  You can run multiple engines, allowing for parallel and distributed computing.\n",
    "\n",
    "**Schedulers**  \n",
    "Any commands that are to be run on an engine first go through a scheduler.  The engines will block when executing code, so the scheduler will manage requests in the background.\n",
    "\n",
    "**Client**  \n",
    "This ia a Python object that lets you connect to a ipyparallel cluster.  \n",
    "**Hub**  \n",
    "The hub is the brain of an ipyparallel cluster.  It manages connections to engines, schedulers, and clients\n",
    "\n",
    "We won't go into all the details of ipyparallel (there's a lot), but we will start a small cluster to run our mpi4py pi code.\n",
    "\n",
    "You can start clusters via command line options:\n",
    "\n",
    "```python\n",
    "ipcluster start -n 4\n",
    "```\n",
    "but we'll do it via a Jupyter notebook extension.  There is an `Ipython Clusters` tab where you can start a cluster.  Select 4 engines and start the cluster:\n",
    "\n",
    "<img src=\"../img/jupyter-cluster.png\" style=\"height:250px\">\n",
    "\n",
    "It will take a few seconds to start up the cluster, but once it's running we can create a client that we'll use to connect to our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "client = ipp.Client()\n",
    "client.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the IDs of the 4 engines we have runninng in our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ipyparallel has a concept called a **view**, which is way to access the engines available.  A **direct** view simply lets you send explicit commands to specific engines (e.g. tell each engine to run the same `compute_pi()` function).  A **load balanced** view is more like the `multiprocessing` module we saw previously; you send a command to a pool of workers, and the scheduler will handle which engine it runs on (depending on which one is available).\n",
    "\n",
    "We're going to just focus on the direct views in this example.  To start we simply define what engines we want to use in our direct view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DirectView [0, 1, 2, 3]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview = client[:]\n",
    "dview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the same pi calculation code we've seen before, but we're going to do in parallel without mpi4py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from math import pi\n",
    "dview['random'] = random\n",
    "\n",
    "# Serial version\n",
    "def serial_pi(nsamples):\n",
    "    s = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random()\n",
    "        y = random()\n",
    "        if x*x + y*y <= 1:\n",
    "            s+=1\n",
    "    return 4.*s/nsamples\n",
    "\n",
    "# Parallel version\n",
    "def parallel_pi(view, nsamples):\n",
    "    p = len(view.targets)\n",
    "    if nsamples % p:\n",
    "        # ensure even divisibility\n",
    "        nsamples += p - (nsamples%p)\n",
    "    \n",
    "    subsamples = nsamples//p\n",
    "    \n",
    "    ar = view.apply(serial_pi, subsamples)\n",
    "    return sum(ar)/p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is similar to the `mpi4py` version:\n",
    "* We define a serial algorithm\n",
    "* We divide up the number of samples based on the number of engines in our view\n",
    "* The serial algorithm is applied across all engines, and the final answer is aggreated.\n",
    "\n",
    "Let's see the serial version performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.06 s ± 237 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "serial_pi(int(1e7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the parallel version, passing in the list of engines in our view that we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72 s ± 50 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "parallel_pi(dview, int(1e7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ipyparallel and clusters\n",
    "\n",
    "ipyparallel has the ability to leverage MPI via mpi4py when communicating with compute engines.  It also is aware of schedulers like SLURM and PBS.  This means you can essentially create a cluster computing environment that is completely usable via a Jupyter notebook.\n",
    "\n",
    "While I wouldn't recommend setting this up on Pawsey HPC systems, it may be a good option for cloud systems.  You can run a remote Jupyter server on a VM, create and configure an IPython cluster, and then use it to schedule and run jobs you've created in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example\n",
    "\n",
    "For those that don't consider Python a viable HPC language, here's an example of what you can do with Python at scale:\n",
    "\n",
    "<img src=\"../img/pyfr_logo.png\" style=\"height:250px\">\n",
    "\n",
    "\n",
    "[PyFR](http://www.pyfr.org/index.php) - A Python framework for solving advection-diffusion problems.\n",
    "\n",
    "Features:\n",
    "* **Multi-platform**\n",
    "    * AMD GPUs\n",
    "    * NVIDIA GPUs\n",
    "    * CPUs\n",
    "    * Intel MIC\n",
    "    * Even Raspberyy Pi  \n",
    "<br>\n",
    "* **Parallelism**\n",
    "    * MPI (mpi4py)\n",
    "    * CUDA (PyCUDA)\n",
    "    * OpenMP (pyMIC for KNL)\n",
    "    * OpenCL (PyOpenCL)\n",
    "    * HDF5 Parallel I/O (h5py)  \n",
    "<br>    \n",
    "* **Scalable**\n",
    "    * 18,000 K20X GPUs on Titan (ORNL)\n",
    "    * 195 billion DOFs\n",
    "    * 58% peak performance (Summit HPL benchmark was 71%)\n",
    "    * SC16 Best Paper/Gordon Bell nominee\n",
    "    \n",
    "And it does all that in about **8000 lines of code**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/pyfr-sim.gif\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluid Flow Demo (from PyFR website)\n",
    "\n",
    "Here we'll run a small PyFR demo, simulation 2D incompressible fluid flow around a cylinder.  We have a separate Conda environment in our notebook for PyFR (select the pyfr kernel from the Kernel menu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/conda-nb.png\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert the mesh file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joyvan/demos/pyfr/inc_cylinder_2d\n"
     ]
    }
   ],
   "source": [
    "%cd /home/joyvan/demos/pyfr/inc_cylinder_2d\n",
    "!pyfr import inc_cylinder_2D.msh inc_cylinder_2D.pyfrm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll the Navier-Stokes solver, and generate a series of output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
      "/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/shapes.py:274: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[G   0.0% [>                             ] 0.01/75.00 ela: 00:00:36 rem: 60:45:19\u001b[2K\u001b[G   0.0% [>                             ] 0.03/75.00 ela: 00:01:11 rem: 59:50:55\u001b[2K\u001b[G   0.1% [>                             ] 0.04/75.00 ela: 00:01:50 rem: 61:31:31\u001b[2K\u001b[G   0.1% [>                             ] 0.05/75.00 ela: 00:02:10 rem: 54:08:43\u001b[2K\u001b[G   0.1% [>                             ] 0.06/75.00 ela: 00:02:17 rem: 45:50:02\u001b[2K\u001b[G   0.1% [>                             ] 0.07/75.00 ela: 00:02:24 rem: 40:12:34\u001b[2K\u001b[G   0.1% [>                             ] 0.09/75.00 ela: 00:02:27 rem: 35:11:32\u001b[2K\u001b[G   0.1% [>                             ] 0.10/75.00 ela: 00:02:28 rem: 30:58:47\u001b[2K\u001b[G   0.1% [>                             ] 0.11/75.00 ela: 00:02:29 rem: 27:38:58\u001b[2K\u001b[G   0.2% [>                             ] 0.12/75.00 ela: 00:02:30 rem: 24:58:18\u001b[2K\u001b[G   0.2% [>                             ] 0.14/75.00 ela: 00:02:30 rem: 22:48:22\u001b[2K\u001b[G   0.2% [>                             ] 0.15/75.00 ela: 00:02:31 rem: 20:58:56\u001b[2K\u001b[G   0.2% [>                             ] 0.16/75.00 ela: 00:02:31 rem: 19:25:30\u001b[2K\u001b[G   0.2% [>                             ] 0.18/75.00 ela: 00:02:34 rem: 18:20:07\u001b[2K\u001b[G   0.3% [>                             ] 0.19/75.00 ela: 00:02:36 rem: 17:22:25\u001b[2K\u001b[G   0.3% [>                             ] 0.20/75.00 ela: 00:02:37 rem: 16:23:18\u001b[2K\u001b[G   0.3% [>                             ] 0.21/75.00 ela: 00:02:39 rem: 15:34:47\u001b[2K\u001b[G   0.3% [>                             ] 0.23/75.00 ela: 00:02:40 rem: 14:51:06\u001b[2K\u001b[G   0.3% [>                             ] 0.24/75.00 ela: 00:02:41 rem: 14:08:05\u001b[2K\u001b[G   0.3% [>                             ] 0.25/75.00 ela: 00:02:42 rem: 13:28:25\u001b[2K\u001b[G   0.4% [>                             ] 0.26/75.00 ela: 00:02:42 rem: 12:51:28\u001b[2K\u001b[G   0.4% [>                             ] 0.28/75.00 ela: 00:02:42 rem: 12:17:44\u001b[2K\u001b[G   0.4% [>                             ] 0.29/75.00 ela: 00:02:43 rem: 11:46:56\u001b[2K\u001b[G   0.4% [>                             ] 0.30/75.00 ela: 00:02:43 rem: 11:18:50\u001b[2K\u001b[G   0.4% [>                             ] 0.31/75.00 ela: 00:02:43 rem: 10:52:57\u001b[2K\u001b[G   0.4% [>                             ] 0.33/75.00 ela: 00:02:44 rem: 10:28:58\u001b[2K\u001b[G   0.5% [>                             ] 0.34/75.00 ela: 00:02:44 rem: 10:06:49\u001b[2K\u001b[G   0.5% [>                             ] 0.35/75.00 ela: 00:02:45 rem: 09:47:09\u001b[2K\u001b[G   0.5% [>                             ] 0.36/75.00 ela: 00:02:45 rem: 09:28:02\u001b[2K\u001b[G   0.5% [>                             ] 0.38/75.00 ela: 00:02:45 rem: 09:10:09\u001b[2K\u001b[G   0.5% [>                             ] 0.39/75.00 ela: 00:02:46 rem: 08:53:23\u001b[2K\u001b[G   0.5% [>                             ] 0.40/75.00 ela: 00:02:46 rem: 08:37:37\u001b[2K\u001b[G   0.6% [>                             ] 0.41/75.00 ela: 00:02:46 rem: 08:22:53\u001b[2K\u001b[G   0.6% [>                             ] 0.43/75.00 ela: 00:02:47 rem: 08:09:02\u001b[2K\u001b[G   0.6% [>                             ] 0.44/75.00 ela: 00:02:47 rem: 07:56:40\u001b[2K\u001b[G   0.6% [>                             ] 0.45/75.00 ela: 00:02:48 rem: 07:45:18\u001b[2K\u001b[G   0.6% [>                             ] 0.46/75.00 ela: 00:02:49 rem: 07:34:56\u001b[2K\u001b[G   0.6% [>                             ] 0.48/75.00 ela: 00:02:50 rem: 07:25:32\u001b[2K\u001b[G   0.7% [>                             ] 0.49/75.00 ela: 00:02:51 rem: 07:15:47\u001b[2K\u001b[G   0.7% [>                             ] 0.50/75.00 ela: 00:02:51 rem: 07:06:22\u001b[2K\u001b[G   0.7% [>                             ] 0.51/75.00 ela: 00:02:52 rem: 06:58:28\u001b[2K\u001b[G   0.7% [>                             ] 0.53/75.00 ela: 00:02:53 rem: 06:50:18\u001b[2K\u001b[G   0.7% [>                             ] 0.54/75.00 ela: 00:02:54 rem: 06:43:01\u001b[2K\u001b[G   0.7% [>                             ] 0.55/75.00 ela: 00:03:00 rem: 06:47:42\u001b[2K\u001b[G   0.8% [>                             ] 0.56/75.00 ela: 00:03:20 rem: 07:22:58\u001b[2K\u001b[G   0.8% [>                             ] 0.57/75.00 ela: 00:03:40 rem: 07:56:31\u001b[2K\u001b[G   0.8% [>                             ] 0.59/75.00 ela: 00:03:46 rem: 07:58:51\u001b[2K\u001b[G   0.8% [>                             ] 0.60/75.00 ela: 00:03:47 rem: 07:49:27\u001b[2K\u001b[G   0.8% [>                             ] 0.61/75.00 ela: 00:03:48 rem: 07:41:40\u001b[2K\u001b[G   0.8% [>                             ] 0.62/75.00 ela: 00:03:48 rem: 07:33:17\u001b[2K\u001b[G   0.8% [>                             ] 0.64/75.00 ela: 00:03:48 rem: 07:25:09\u001b[2K\u001b[G   0.9% [>                             ] 0.65/75.00 ela: 00:03:49 rem: 07:17:08\u001b[2K\u001b[G   0.9% [>                             ] 0.66/75.00 ela: 00:03:49 rem: 07:09:26\u001b[2K\u001b[G   0.9% [>                             ] 0.67/75.00 ela: 00:03:49 rem: 07:02:00\u001b[2K\u001b[G   0.9% [>                             ] 0.69/75.00 ela: 00:03:50 rem: 06:54:50\u001b[2K\u001b[G   0.9% [>                             ] 0.70/75.00 ela: 00:03:50 rem: 06:47:55\u001b[2K\u001b[G   0.9% [>                             ] 0.71/75.00 ela: 00:03:50 rem: 06:41:13\u001b[2K\u001b[G   1.0% [>                             ] 0.72/75.00 ela: 00:03:51 rem: 06:34:48\u001b[2K\u001b[G   1.0% [>                             ] 0.74/75.00 ela: 00:03:51 rem: 06:28:39\u001b[2K\u001b[G   1.0% [>                             ] 0.75/75.00 ela: 00:03:51 rem: 06:22:41\u001b[2K\u001b[G   1.0% [>                             ] 0.76/75.00 ela: 00:03:52 rem: 06:16:56\u001b[2K\u001b[G   1.0% [>                             ] 0.77/75.00 ela: 00:03:52 rem: 06:11:23\u001b[2K\u001b[G   1.0% [>                             ] 0.79/75.00 ela: 00:03:53 rem: 06:05:59\u001b[2K\u001b[G   1.1% [>                             ] 0.80/75.00 ela: 00:03:53 rem: 06:00:44\u001b[2K\u001b[G   1.1% [>                             ] 0.81/75.00 ela: 00:03:53 rem: 05:55:40\u001b[2K\u001b[G   1.1% [>                             ] 0.82/75.00 ela: 00:03:54 rem: 05:50:45\u001b[2K\u001b[G   1.1% [>                             ] 0.84/75.00 ela: 00:03:54 rem: 05:45:58\u001b[2K\u001b[G   1.1% [>                             ] 0.85/75.00 ela: 00:03:54 rem: 05:41:21\u001b[2K\u001b[G   1.1% [>                             ] 0.86/75.00 ela: 00:03:55 rem: 05:36:52\u001b[2K\u001b[G   1.2% [>                             ] 0.87/75.00 ela: 00:03:55 rem: 05:32:29\u001b[2K\u001b[G   1.2% [>                             ] 0.89/75.00 ela: 00:03:55 rem: 05:28:16\u001b[2K\u001b[G   1.2% [>                             ] 0.90/75.00 ela: 00:03:56 rem: 05:24:09\u001b[2K\u001b[G   1.2% [>                             ] 0.91/75.00 ela: 00:03:56 rem: 05:20:09\u001b[2K\u001b[G   1.2% [>                             ] 0.92/75.00 ela: 00:03:56 rem: 05:16:16\u001b[2K\u001b[G   1.2% [>                             ] 0.94/75.00 ela: 00:03:57 rem: 05:12:29\u001b[2K\u001b[G   1.3% [>                             ] 0.95/75.00 ela: 00:03:57 rem: 05:08:48\u001b[2K\u001b[G   1.3% [>                             ] 0.96/75.00 ela: 00:03:58 rem: 05:05:13\u001b[2K\u001b[G   1.3% [>                             ] 0.97/75.00 ela: 00:03:58 rem: 05:01:43\u001b[2K\u001b[G   1.3% [>                             ] 0.99/75.00 ela: 00:03:58 rem: 04:58:20\u001b[2K\u001b[G   1.3% [>                             ] 1.00/75.00 ela: 00:03:59 rem: 04:55:00\u001b[2K\u001b[G   1.3% [>                             ] 1.01/75.00 ela: 00:03:59 rem: 04:51:46\u001b[2K\u001b[G   1.4% [>                             ] 1.02/75.00 ela: 00:03:59 rem: 04:48:40\u001b[2K\u001b[G   1.4% [>                             ] 1.04/75.00 ela: 00:04:00 rem: 04:45:35\u001b[2K\u001b[G   1.4% [>                             ] 1.05/75.00 ela: 00:04:00 rem: 04:42:33\u001b[2K\u001b[G   1.4% [>                             ] 1.06/75.00 ela: 00:04:01 rem: 04:39:39\u001b[2K\u001b[G   1.4% [>                             ] 1.07/75.00 ela: 00:04:01 rem: 04:36:48\u001b[2K\u001b[G   1.4% [>                             ] 1.09/75.00 ela: 00:04:01 rem: 04:34:02\u001b[2K\u001b[G   1.5% [>                             ] 1.10/75.00 ela: 00:04:02 rem: 04:31:19\u001b[2K\u001b[G   1.5% [>                             ] 1.11/75.00 ela: 00:04:02 rem: 04:28:39\u001b[2K\u001b[G   1.5% [>                             ] 1.12/75.00 ela: 00:04:03 rem: 04:26:07\u001b[2K\u001b[G   1.5% [>                             ] 1.14/75.00 ela: 00:04:03 rem: 04:23:48\u001b[2K\u001b[G   1.5% [>                             ] 1.15/75.00 ela: 00:04:04 rem: 04:21:40\u001b[2K\u001b[G   1.5% [>                             ] 1.16/75.00 ela: 00:04:06 rem: 04:20:26\u001b[2K\u001b[G   1.6% [>                             ] 1.17/75.00 ela: 00:04:10 rem: 04:21:59\u001b[2K\u001b[G   1.6% [>                             ] 1.19/75.00 ela: 00:04:15 rem: 04:24:44\u001b[2K\u001b[G   1.6% [>                             ] 1.20/75.00 ela: 00:04:18 rem: 04:24:33"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[G   1.6% [>                             ] 1.21/75.00 ela: 00:04:26 rem: 04:30:19\u001b[2K\u001b[G   1.6% [>                             ] 1.22/75.00 ela: 00:04:35 rem: 04:36:02\u001b[2K\u001b[G   1.6% [>                             ] 1.24/75.00 ela: 00:04:41 rem: 04:39:34\u001b[2K\u001b[G   1.7% [>                             ] 1.25/75.00 ela: 00:04:49 rem: 04:44:52\u001b[2K\u001b[G   1.7% [>                             ] 1.26/75.00 ela: 00:04:55 rem: 04:47:23\u001b[2K\u001b[G   1.7% [>                             ] 1.27/75.00 ela: 00:05:02 rem: 04:51:21\u001b[2K\u001b[G   1.7% [>                             ] 1.29/75.00 ela: 00:05:06 rem: 04:52:24\u001b[2K\u001b[G   1.7% [=>                            ] 1.30/75.00 ela: 00:05:10 rem: 04:53:47\u001b[2K\u001b[G   1.7% [=>                            ] 1.31/75.00 ela: 00:05:16 rem: 04:56:32\u001b[2K\u001b[G   1.8% [=>                            ] 1.32/75.00 ela: 00:05:21 rem: 04:58:06\u001b[2K\u001b[G   1.8% [=>                            ] 1.34/75.00 ela: 00:05:24 rem: 04:58:04\u001b[2K\u001b[G   1.8% [=>                            ] 1.35/75.00 ela: 00:05:29 rem: 04:59:14\u001b[2K\u001b[G   1.8% [=>                            ] 1.36/75.00 ela: 00:05:32 rem: 04:59:14\u001b[2K\u001b[G   1.8% [=>                            ] 1.37/75.00 ela: 00:05:36 rem: 05:00:02\u001b[2K\u001b[G   1.8% [=>                            ] 1.39/75.00 ela: 00:05:41 rem: 05:01:40\u001b[2K\u001b[G   1.9% [=>                            ] 1.40/75.00 ela: 00:05:46 rem: 05:03:58\u001b[2K\u001b[G   1.9% [=>                            ] 1.41/75.00 ela: 00:05:55 rem: 05:08:34\u001b[2K\u001b[G   1.9% [=>                            ] 1.42/75.00 ela: 00:05:58 rem: 05:08:15\u001b[2K\u001b[G   1.9% [=>                            ] 1.44/75.00 ela: 00:06:01 rem: 05:08:40\u001b[2K\u001b[G   1.9% [=>                            ] 1.45/75.00 ela: 00:06:05 rem: 05:08:52\u001b[2K\u001b[G   1.9% [=>                            ] 1.46/75.00 ela: 00:06:10 rem: 05:10:27\u001b[2K\u001b[G   2.0% [=>                            ] 1.47/75.00 ela: 00:06:12 rem: 05:09:51\u001b[2K\u001b[G   2.0% [=>                            ] 1.49/75.00 ela: 00:06:16 rem: 05:10:02\u001b[2K\u001b[G   2.0% [=>                            ] 1.50/75.00 ela: 00:06:21 rem: 05:11:44\u001b[2K\u001b[G   2.0% [=>                            ] 1.51/75.00 ela: 00:06:25 rem: 05:12:24\u001b[2K\u001b[G   2.0% [=>                            ] 1.52/75.00 ela: 00:06:28 rem: 05:11:58\u001b[2K\u001b[G   2.0% [=>                            ] 1.54/75.00 ela: 00:06:32 rem: 05:12:16\u001b[2K\u001b[G   2.1% [=>                            ] 1.55/75.00 ela: 00:06:38 rem: 05:14:31\u001b[2K\u001b[G   2.1% [=>                            ] 1.56/75.00 ela: 00:06:40 rem: 05:13:29\u001b[2K\u001b[G   2.1% [=>                            ] 1.57/75.00 ela: 00:06:45 rem: 05:14:57\u001b[2K\u001b[G   2.1% [=>                            ] 1.59/75.00 ela: 00:06:48 rem: 05:14:33\u001b[2K\u001b[G   2.1% [=>                            ] 1.60/75.00 ela: 00:06:52 rem: 05:15:28\u001b[2K\u001b[G   2.1% [=>                            ] 1.61/75.00 ela: 00:06:57 rem: 05:16:34\u001b[2K\u001b[G   2.2% [=>                            ] 1.62/75.00 ela: 00:07:01 rem: 05:16:54\u001b[2K\u001b[G   2.2% [=>                            ] 1.64/75.00 ela: 00:07:03 rem: 05:15:55\u001b[2K\u001b[G   2.2% [=>                            ] 1.65/75.00 ela: 00:07:06 rem: 05:16:20\u001b[2K\u001b[G   2.2% [=>                            ] 1.66/75.00 ela: 00:07:11 rem: 05:16:53\u001b[2K\u001b[G   2.2% [=>                            ] 1.67/75.00 ela: 00:07:12 rem: 05:15:45\u001b[2K\u001b[G   2.2% [=>                            ] 1.69/75.00 ela: 00:07:14 rem: 05:14:27\u001b[2K\u001b[G   2.3% [=>                            ] 1.70/75.00 ela: 00:07:18 rem: 05:14:57\u001b[2K\u001b[G   2.3% [=>                            ] 1.71/75.00 ela: 00:07:22 rem: 05:15:27\u001b[2K\u001b[G   2.3% [=>                            ] 1.72/75.00 ela: 00:07:25 rem: 05:15:37\u001b[2K\u001b[G   2.3% [=>                            ] 1.74/75.00 ela: 00:07:28 rem: 05:15:00\u001b[2K\u001b[G   2.3% [=>                            ] 1.75/75.00 ela: 00:07:30 rem: 05:14:17\u001b[2K\u001b[G   2.3% [=>                            ] 1.76/75.00 ela: 00:07:34 rem: 05:14:48\u001b[2K\u001b[G   2.4% [=>                            ] 1.77/75.00 ela: 00:07:37 rem: 05:14:12\u001b[2K\u001b[G   2.4% [=>                            ] 1.79/75.00 ela: 00:07:44 rem: 05:16:51\u001b[2K\u001b[G   2.4% [=>                            ] 1.80/75.00 ela: 00:07:49 rem: 05:18:27\u001b[2K\u001b[G   2.4% [=>                            ] 1.81/75.00 ela: 00:07:54 rem: 05:19:07\u001b[2K\u001b[G   2.4% [=>                            ] 1.82/75.00 ela: 00:07:55 rem: 05:18:03\u001b[2K\u001b[G   2.4% [=>                            ] 1.84/75.00 ela: 00:07:58 rem: 05:17:30\u001b[2K\u001b[G   2.5% [=>                            ] 1.85/75.00 ela: 00:07:59 rem: 05:16:13\u001b[2K\u001b[G   2.5% [=>                            ] 1.86/75.00 ela: 00:08:05 rem: 05:17:26\u001b[2K\u001b[G   2.5% [=>                            ] 1.87/75.00 ela: 00:08:09 rem: 05:18:13\u001b[2K\u001b[G   2.5% [=>                            ] 1.89/75.00 ela: 00:08:12 rem: 05:17:55\u001b[2K\u001b[G   2.5% [=>                            ] 1.90/75.00 ela: 00:08:15 rem: 05:17:40\u001b[2K\u001b[G   2.5% [=>                            ] 1.91/75.00 ela: 00:08:16 rem: 05:16:11\u001b[2K\u001b[G   2.6% [=>                            ] 1.92/75.00 ela: 00:08:18 rem: 05:15:41\u001b[2K\u001b[G   2.6% [=>                            ] 1.94/75.00 ela: 00:08:22 rem: 05:16:03\u001b[2K\u001b[G   2.6% [=>                            ] 1.95/75.00 ela: 00:08:24 rem: 05:15:16\u001b[2K\u001b[G   2.6% [=>                            ] 1.96/75.00 ela: 00:08:25 rem: 05:13:50\u001b[2K\u001b[G   2.6% [=>                            ] 1.97/75.00 ela: 00:08:29 rem: 05:13:46\u001b[2K\u001b[G   2.6% [=>                            ] 1.99/75.00 ela: 00:08:33 rem: 05:14:20\u001b[2K\u001b[G   2.7% [=>                            ] 2.00/75.00 ela: 00:08:37 rem: 05:14:35\u001b[2K\u001b[G   2.7% [=>                            ] 2.01/75.00 ela: 00:08:38 rem: 05:13:14\u001b[2K\u001b[G   2.7% [=>                            ] 2.02/75.00 ela: 00:08:40 rem: 05:12:22\u001b[2K\u001b[G   2.7% [=>                            ] 2.04/75.00 ela: 00:08:40 rem: 05:10:47\u001b[2K\u001b[G   2.7% [=>                            ] 2.05/75.00 ela: 00:08:43 rem: 05:10:27\u001b[2K\u001b[G   2.7% [=>                            ] 2.06/75.00 ela: 00:08:44 rem: 05:09:09\u001b[2K\u001b[G   2.8% [=>                            ] 2.07/75.00 ela: 00:08:47 rem: 05:08:59\u001b[2K\u001b[G   2.8% [=>                            ] 2.09/75.00 ela: 00:08:50 rem: 05:08:56\u001b[2K\u001b[G   2.8% [=>                            ] 2.10/75.00 ela: 00:08:55 rem: 05:09:42\u001b[2K\u001b[G   2.8% [=>                            ] 2.11/75.00 ela: 00:08:56 rem: 05:08:24\u001b[2K\u001b[G   2.8% [=>                            ] 2.12/75.00 ela: 00:08:58 rem: 05:07:39\u001b[2K\u001b[G   2.8% [=>                            ] 2.14/75.00 ela: 00:09:00 rem: 05:06:56\u001b[2K\u001b[G   2.9% [=>                            ] 2.15/75.00 ela: 00:09:05 rem: 05:07:50\u001b[2K\u001b[G   2.9% [=>                            ] 2.16/75.00 ela: 00:09:05 rem: 05:06:10\u001b[2K\u001b[G   2.9% [=>                            ] 2.17/75.00 ela: 00:09:05 rem: 05:04:30\u001b[2K\u001b[G   2.9% [=>                            ] 2.19/75.00 ela: 00:09:05 rem: 05:02:52\u001b[2K\u001b[G   2.9% [=>                            ] 2.20/75.00 ela: 00:09:06 rem: 05:01:14\u001b[2K\u001b[G   2.9% [=>                            ] 2.21/75.00 ela: 00:09:06 rem: 04:59:38\u001b[2K\u001b[G   3.0% [=>                            ] 2.22/75.00 ela: 00:09:11 rem: 05:00:29\u001b[2K\u001b[G   3.0% [=>                            ] 2.24/75.00 ela: 00:09:47 rem: 05:18:11\u001b[2K\u001b[G   3.0% [=>                            ] 2.25/75.00 ela: 00:09:57 rem: 05:22:02\u001b[2K\u001b[G   3.0% [=>                            ] 2.26/75.00 ela: 00:09:59 rem: 05:21:04\u001b[2K\u001b[G   3.0% [=>                            ] 2.27/75.00 ela: 00:10:03 rem: 05:21:23\u001b[2K\u001b[G   3.0% [=>                            ] 2.29/75.00 ela: 00:10:05 rem: 05:20:57\u001b[2K\u001b[G   3.1% [=>                            ] 2.30/75.00 ela: 00:10:24 rem: 05:28:46\u001b[2K\u001b[G   3.1% [=>                            ] 2.31/75.00 ela: 00:10:30 rem: 05:30:27\u001b[2K\u001b[G   3.1% [=>                            ] 2.32/75.00 ela: 00:10:34 rem: 05:30:40\u001b[2K\u001b[G   3.1% [=>                            ] 2.34/75.00 ela: 00:10:38 rem: 05:30:36\u001b[2K\u001b[G   3.1% [=>                            ] 2.35/75.00 ela: 00:10:40 rem: 05:30:07\u001b[2K\u001b[G   3.1% [=>                            ] 2.36/75.00 ela: 00:11:05 rem: 05:41:08\u001b[2K\u001b[G   3.2% [=>                            ] 2.38/75.00 ela: 00:11:08 rem: 05:40:42\u001b[2K\u001b[G   3.2% [=>                            ] 2.39/75.00 ela: 00:11:11 rem: 05:40:25\u001b[2K\u001b[G   3.2% [=>                            ] 2.40/75.00 ela: 00:11:15 rem: 05:40:20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[G   3.2% [=>                            ] 2.41/75.00 ela: 00:11:23 rem: 05:42:41\u001b[2K\u001b[G   3.2% [=>                            ] 2.43/75.00 ela: 00:11:47 rem: 05:52:47\u001b[2K\u001b[G   3.3% [=>                            ] 2.44/75.00 ela: 00:11:51 rem: 05:52:53\u001b[2K\u001b[G   3.3% [=>                            ] 2.45/75.00 ela: 00:11:53 rem: 05:52:08\u001b[2K\u001b[G   3.3% [=>                            ] 2.46/75.00 ela: 00:11:59 rem: 05:53:21\u001b[2K\u001b[G   3.3% [=>                            ] 2.48/75.00 ela: 00:12:10 rem: 05:57:00\u001b[2K\u001b[G   3.3% [=>                            ] 2.49/75.00 ela: 00:12:28 rem: 06:03:31\u001b[2K\u001b[G   3.3% [=>                            ] 2.50/75.00 ela: 00:12:39 rem: 06:07:06\u001b[2K\u001b[G   3.4% [=>                            ] 2.51/75.00 ela: 00:13:02 rem: 06:16:15\u001b[2K\u001b[G   3.4% [=>                            ] 2.53/75.00 ela: 00:13:13 rem: 06:19:31\u001b[2K\u001b[G   3.4% [=>                            ] 2.54/75.00 ela: 00:13:17 rem: 06:19:42\u001b[2K\u001b[G   3.4% [=>                            ] 2.55/75.00 ela: 00:13:21 rem: 06:19:31\u001b[2K\u001b[G   3.4% [=>                            ] 2.56/75.00 ela: 00:13:37 rem: 06:25:02\u001b[2K\u001b[G   3.4% [=>                            ] 2.58/75.00 ela: 00:13:40 rem: 06:24:49\u001b[2K\u001b[G   3.5% [=>                            ] 2.59/75.00 ela: 00:13:43 rem: 06:24:13\u001b[2K\u001b[G   3.5% [=>                            ] 2.60/75.00 ela: 00:13:48 rem: 06:24:19\u001b[2K\u001b[G   3.5% [=>                            ] 2.61/75.00 ela: 00:13:51 rem: 06:24:03\u001b[2K\u001b[G   3.5% [=>                            ] 2.63/75.00 ela: 00:14:03 rem: 06:27:46\u001b[2K\u001b[G   3.5% [=>                            ] 2.64/75.00 ela: 00:14:05 rem: 06:26:38\u001b[2K\u001b[G   3.5% [=>                            ] 2.65/75.00 ela: 00:14:14 rem: 06:28:39\u001b[2K\u001b[G   3.6% [=>                            ] 2.66/75.00 ela: 00:14:31 rem: 06:34:39\u001b[2K\u001b[G   3.6% [=>                            ] 2.68/75.00 ela: 00:14:52 rem: 06:42:00\u001b[2K\u001b[G   3.6% [=>                            ] 2.69/75.00 ela: 00:15:06 rem: 06:46:37\u001b[2K\u001b[G   3.6% [=>                            ] 2.70/75.00 ela: 00:15:07 rem: 06:45:13\u001b[2K\u001b[G   3.6% [=>                            ] 2.71/75.00 ela: 00:15:08 rem: 06:43:29\u001b[2K\u001b[G   3.6% [=>                            ] 2.73/75.00 ela: 00:15:08 rem: 06:41:45\u001b[2K\u001b[G   3.7% [=>                            ] 2.74/75.00 ela: 00:15:09 rem: 06:40:01\u001b[2K\u001b[G   3.7% [=>                            ] 2.75/75.00 ela: 00:15:09 rem: 06:38:18\u001b[2K\u001b[G   3.7% [=>                            ] 2.76/75.00 ela: 00:15:10 rem: 06:36:38\u001b[2K\u001b[G   3.7% [=>                            ] 2.78/75.00 ela: 00:15:10 rem: 06:34:59\u001b[2K\u001b[G   3.7% [=>                            ] 2.79/75.00 ela: 00:15:11 rem: 06:33:20\u001b[2K\u001b[G   3.7% [=>                            ] 2.80/75.00 ela: 00:15:11 rem: 06:31:43\u001b[2K\u001b[G   3.8% [=>                            ] 2.81/75.00 ela: 00:15:11 rem: 06:30:05\u001b[2K\u001b[G   3.8% [=>                            ] 2.83/75.00 ela: 00:15:12 rem: 06:28:28\u001b[2K\u001b[G   3.8% [=>                            ] 2.84/75.00 ela: 00:15:12 rem: 06:26:51\u001b[2K\u001b[G   3.8% [=>                            ] 2.85/75.00 ela: 00:15:13 rem: 06:25:16\u001b[2K\u001b[G   3.8% [=>                            ] 2.86/75.00 ela: 00:15:13 rem: 06:23:42\u001b[2K\u001b[G   3.8% [=>                            ] 2.88/75.00 ela: 00:15:14 rem: 06:22:10\u001b[2K\u001b[G   3.9% [=>                            ] 2.89/75.00 ela: 00:15:14 rem: 06:20:36\u001b[2K\u001b[G   3.9% [=>                            ] 2.90/75.00 ela: 00:15:14 rem: 06:19:06\u001b[2K\u001b[G   3.9% [=>                            ] 2.91/75.00 ela: 00:15:15 rem: 06:17:36\u001b[2K\u001b[G   3.9% [=>                            ] 2.93/75.00 ela: 00:15:16 rem: 06:16:17\u001b[2K\u001b[G   3.9% [=>                            ] 2.94/75.00 ela: 00:15:16 rem: 06:14:54\u001b[2K\u001b[G   3.9% [=>                            ] 2.95/75.00 ela: 00:15:17 rem: 06:13:25\u001b[2K\u001b[G   4.0% [=>                            ] 2.96/75.00 ela: 00:15:18 rem: 06:12:09\u001b[2K\u001b[G   4.0% [=>                            ] 2.98/75.00 ela: 00:15:19 rem: 06:10:58\u001b[2K\u001b[G   4.0% [=>                            ] 2.99/75.00 ela: 00:15:23 rem: 06:10:54\u001b[2K\u001b[G   4.0% [=>                            ] 3.00/75.00 ela: 00:15:26 rem: 06:10:24\u001b[2K\u001b[G   4.0% [=>                            ] 3.01/75.00 ela: 00:15:26 rem: 06:09:06\u001b[2K\u001b[G   4.0% [=>                            ] 3.03/75.00 ela: 00:15:27 rem: 06:07:50\u001b[2K\u001b[G   4.1% [=>                            ] 3.04/75.00 ela: 00:15:28 rem: 06:06:34\u001b[2K\u001b[G   4.1% [=>                            ] 3.05/75.00 ela: 00:15:28 rem: 06:05:11\u001b[2K\u001b[G   4.1% [=>                            ] 3.06/75.00 ela: 00:15:29 rem: 06:03:54\u001b[2K\u001b[G   4.1% [=>                            ] 3.08/75.00 ela: 00:15:31 rem: 06:03:02\u001b[2K\u001b[G   4.1% [=>                            ] 3.09/75.00 ela: 00:15:32 rem: 06:02:03\u001b[2K\u001b[G   4.1% [=>                            ] 3.10/75.00 ela: 00:15:33 rem: 06:00:58\u001b[2K\u001b[G   4.2% [=>                            ] 3.11/75.00 ela: 00:15:34 rem: 05:59:51\u001b[2K\u001b[G   4.2% [=>                            ] 3.13/75.00 ela: 00:15:36 rem: 05:58:52\u001b[2K\u001b[G   4.2% [=>                            ] 3.14/75.00 ela: 00:15:37 rem: 05:57:50\u001b[2K\u001b[G   4.2% [=>                            ] 3.15/75.00 ela: 00:15:38 rem: 05:56:41\u001b[2K\u001b[G   4.2% [=>                            ] 3.16/75.00 ela: 00:15:39 rem: 05:55:30\u001b[2K\u001b[G   4.2% [=>                            ] 3.18/75.00 ela: 00:15:41 rem: 05:54:54\u001b[2K\u001b[G   4.3% [=>                            ] 3.19/75.00 ela: 00:15:43 rem: 05:54:05\u001b[2K\u001b[G   4.3% [=>                            ] 3.20/75.00 ela: 00:15:44 rem: 05:53:04\u001b[2K\u001b[G   4.3% [=>                            ] 3.21/75.00 ela: 00:15:45 rem: 05:52:14\u001b[2K\u001b[G   4.3% [=>                            ] 3.23/75.00 ela: 00:15:47 rem: 05:51:31\u001b[2K\u001b[G   4.3% [=>                            ] 3.24/75.00 ela: 00:15:49 rem: 05:50:40\u001b[2K\u001b[G   4.3% [=>                            ] 3.25/75.00 ela: 00:15:50 rem: 05:49:36\u001b[2K\u001b[G   4.4% [=>                            ] 3.26/75.00 ela: 00:15:51 rem: 05:48:52\u001b[2K\u001b[G   4.4% [=>                            ] 3.28/75.00 ela: 00:15:54 rem: 05:48:28\u001b[2K\u001b[G   4.4% [=>                            ] 3.29/75.00 ela: 00:15:56 rem: 05:47:44\u001b[2K\u001b[G   4.4% [=>                            ] 3.30/75.00 ela: 00:15:58 rem: 05:47:15\u001b[2K\u001b[G   4.4% [=>                            ] 3.31/75.00 ela: 00:16:00 rem: 05:46:34\u001b[2K\u001b[G   4.4% [=>                            ] 3.33/75.00 ela: 00:16:03 rem: 05:45:59\u001b[2K\u001b[G   4.5% [=>                            ] 3.34/75.00 ela: 00:16:04 rem: 05:45:03\u001b[2K\u001b[G   4.5% [=>                            ] 3.35/75.00 ela: 00:16:05 rem: 05:44:19\u001b[2K\u001b[G   4.5% [=>                            ] 3.36/75.00 ela: 00:16:07 rem: 05:43:35\u001b[2K\u001b[G   4.5% [=>                            ] 3.38/75.00 ela: 00:16:08 rem: 05:42:44\u001b[2K\u001b[G   4.5% [=>                            ] 3.39/75.00 ela: 00:16:09 rem: 05:41:35\u001b[2K\u001b[G   4.5% [=>                            ] 3.40/75.00 ela: 00:16:10 rem: 05:40:31\u001b[2K\u001b[G   4.6% [=>                            ] 3.41/75.00 ela: 00:16:10 rem: 05:39:21\u001b[2K\u001b[G   4.6% [=>                            ] 3.43/75.00 ela: 00:16:10 rem: 05:38:11\u001b[2K\u001b[G   4.6% [=>                            ] 3.44/75.00 ela: 00:16:11 rem: 05:37:03\u001b[2K\u001b[G   4.6% [=>                            ] 3.45/75.00 ela: 00:16:11 rem: 05:35:53\u001b[2K\u001b[G   4.6% [=>                            ] 3.46/75.00 ela: 00:16:12 rem: 05:34:44\u001b[2K\u001b[G   4.6% [=>                            ] 3.48/75.00 ela: 00:16:12 rem: 05:33:36\u001b[2K\u001b[G   4.7% [=>                            ] 3.49/75.00 ela: 00:16:12 rem: 05:32:30\u001b[2K\u001b[G   4.7% [=>                            ] 3.50/75.00 ela: 00:16:13 rem: 05:31:25\u001b[2K\u001b[G   4.7% [=>                            ] 3.51/75.00 ela: 00:16:13 rem: 05:30:21\u001b[2K\u001b[G   4.7% [=>                            ] 3.53/75.00 ela: 00:16:14 rem: 05:29:15\u001b[2K\u001b[G   4.7% [=>                            ] 3.54/75.00 ela: 00:16:14 rem: 05:28:09\u001b[2K\u001b[G   4.7% [=>                            ] 3.55/75.00 ela: 00:16:15 rem: 05:27:12\u001b[2K\u001b[G   4.8% [=>                            ] 3.56/75.00 ela: 00:16:16 rem: 05:26:11\u001b[2K\u001b[G   4.8% [=>                            ] 3.58/75.00 ela: 00:16:16 rem: 05:25:14\u001b[2K\u001b[G   4.8% [=>                            ] 3.59/75.00 ela: 00:16:17 rem: 05:24:25\u001b[2K\u001b[G   4.8% [=>                            ] 3.60/75.00 ela: 00:16:19 rem: 05:23:47"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[G   4.8% [=>                            ] 3.61/75.00 ela: 00:16:20 rem: 05:22:46\u001b[2K\u001b[G   4.8% [=>                            ] 3.63/75.00 ela: 00:16:23 rem: 05:22:49\u001b[2K\u001b[G   4.9% [=>                            ] 3.64/75.00 ela: 00:16:46 rem: 05:29:08\u001b[2K\u001b[G   4.9% [=>                            ] 3.65/75.00 ela: 00:17:05 rem: 05:34:02\u001b[2K\u001b[G   4.9% [=>                            ] 3.66/75.00 ela: 00:17:28 rem: 05:40:15\u001b[2K\u001b[G   4.9% [=>                            ] 3.68/75.00 ela: 00:17:51 rem: 05:46:29\u001b[2K\u001b[G   4.9% [=>                            ] 3.69/75.00 ela: 00:18:13 rem: 05:52:32\u001b[2K\u001b[G   4.9% [=>                            ] 3.70/75.00 ela: 00:18:28 rem: 05:55:54\u001b[2K\u001b[G   5.0% [=>                            ] 3.71/75.00 ela: 00:18:35 rem: 05:57:03\u001b[2K\u001b[G   5.0% [=>                            ] 3.73/75.00 ela: 00:18:41 rem: 05:57:39\u001b[2K\u001b[G   5.0% [=>                            ] 3.74/75.00 ela: 00:18:42 rem: 05:56:42\u001b[2K\u001b[G   5.0% [=>                            ] 3.75/75.00 ela: 00:18:43 rem: 05:55:51\u001b[2K\u001b[G   5.0% [=>                            ] 3.76/75.00 ela: 00:18:44 rem: 05:54:49\u001b[2K\u001b[G   5.0% [=>                            ] 3.78/75.00 ela: 00:18:45 rem: 05:53:46\u001b[2K\u001b[G   5.1% [=>                            ] 3.79/75.00 ela: 00:18:46 rem: 05:53:05\u001b[2K\u001b[G   5.1% [=>                            ] 3.80/75.00 ela: 00:18:47 rem: 05:52:03\u001b[2K\u001b[G   5.1% [=>                            ] 3.81/75.00 ela: 00:18:47 rem: 05:51:00\u001b[2K\u001b[G   5.1% [=>                            ] 3.83/75.00 ela: 00:18:48 rem: 05:50:01\u001b[2K\u001b[G   5.1% [=>                            ] 3.84/75.00 ela: 00:18:49 rem: 05:48:59\u001b[2K\u001b[G   5.1% [=>                            ] 3.85/75.00 ela: 00:18:49 rem: 05:47:54\u001b[2K\u001b[G   5.2% [=>                            ] 3.86/75.00 ela: 00:18:50 rem: 05:46:51\u001b[2K\u001b[G   5.2% [=>                            ] 3.88/75.00 ela: 00:18:50 rem: 05:45:48\u001b[2K\u001b[G   5.2% [==>                           ] 3.89/75.00 ela: 00:18:51 rem: 05:44:52\u001b[2K\u001b[G   5.2% [==>                           ] 3.90/75.00 ela: 00:18:51 rem: 05:43:53\u001b[2K\u001b[G   5.2% [==>                           ] 3.91/75.00 ela: 00:18:52 rem: 05:42:56\u001b[2K\u001b[G   5.2% [==>                           ] 3.93/75.00 ela: 00:18:53 rem: 05:42:00\u001b[2K\u001b[G   5.3% [==>                           ] 3.94/75.00 ela: 00:18:53 rem: 05:41:05\u001b[2K\u001b[G   5.3% [==>                           ] 3.95/75.00 ela: 00:18:54 rem: 05:40:08\u001b[2K\u001b[G   5.3% [==>                           ] 3.96/75.00 ela: 00:18:55 rem: 05:39:14\u001b[2K\u001b[G   5.3% [==>                           ] 3.98/75.00 ela: 00:18:56 rem: 05:38:19\u001b[2K\u001b[G   5.3% [==>                           ] 3.99/75.00 ela: 00:18:56 rem: 05:37:20\u001b[2K\u001b[G   5.3% [==>                           ] 4.00/75.00 ela: 00:18:57 rem: 05:36:22\u001b[2K\u001b[G   5.4% [==>                           ] 4.01/75.00 ela: 00:18:57 rem: 05:35:24\u001b[2K\u001b[G   5.4% [==>                           ] 4.03/75.00 ela: 00:18:57 rem: 05:34:24\u001b[2K\u001b[G   5.4% [==>                           ] 4.04/75.00 ela: 00:18:58 rem: 05:33:25\u001b[2K\u001b[G   5.4% [==>                           ] 4.05/75.00 ela: 00:18:58 rem: 05:32:28\u001b[2K\u001b[G   5.4% [==>                           ] 4.06/75.00 ela: 00:18:59 rem: 05:31:30\u001b[2K\u001b[G   5.4% [==>                           ] 4.08/75.00 ela: 00:18:59 rem: 05:30:33\u001b[2K\u001b[G   5.5% [==>                           ] 4.09/75.00 ela: 00:19:00 rem: 05:29:38\u001b[2K\u001b[G   5.5% [==>                           ] 4.10/75.00 ela: 00:19:00 rem: 05:28:41\u001b[2K\u001b[G   5.5% [==>                           ] 4.11/75.00 ela: 00:19:00 rem: 05:27:44\u001b[2K\u001b[G   5.5% [==>                           ] 4.13/75.00 ela: 00:19:01 rem: 05:26:48\u001b[2K\u001b[G   5.5% [==>                           ] 4.14/75.00 ela: 00:19:01 rem: 05:25:53\u001b[2K\u001b[G   5.5% [==>                           ] 4.15/75.00 ela: 00:19:02 rem: 05:25:00\u001b[2K\u001b[G   5.6% [==>                           ] 4.16/75.00 ela: 00:19:02 rem: 05:24:07\u001b[2K\u001b[G   5.6% [==>                           ] 4.18/75.00 ela: 00:19:03 rem: 05:23:16\u001b[2K\u001b[G   5.6% [==>                           ] 4.19/75.00 ela: 00:19:04 rem: 05:22:27\u001b[2K\u001b[G   5.6% [==>                           ] 4.20/75.00 ela: 00:19:04 rem: 05:21:35\u001b[2K\u001b[G   5.6% [==>                           ] 4.21/75.00 ela: 00:19:05 rem: 05:20:44\u001b[2K\u001b[G   5.6% [==>                           ] 4.23/75.00 ela: 00:19:05 rem: 05:19:50\u001b[2K\u001b[G   5.7% [==>                           ] 4.24/75.00 ela: 00:19:06 rem: 05:18:57"
     ]
    }
   ],
   "source": [
    "!pyfr run -b openmp -p inc_cylinder_2D.pyfrm inc_cylinder_2d.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joyvan/demos/pyfr/inc_cylinder_2d\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pyfr/bin/pyfr\", line 11, in <module>\n",
      "    load_entry_point('pyfr==1.8.0', 'console_scripts', 'pyfr')()\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 110, in main\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/__main__.py\", line 187, in process_export\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/writers/__init__.py\", line 18, in get_writer_by_extn\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/writers/vtk.py\", line 20, in __init__\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/writers/base.py\", line 15, in __init__\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/pyfr-1.8.0-py3.7.egg/pyfr/readers/native.py\", line 16, in __init__\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/files.py\", line 394, in __init__\n",
      "    swmr=swmr)\n",
      "  File \"/opt/conda/envs/pyfr/lib/python3.7/site-packages/h5py/_hl/files.py\", line 170, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 85, in h5py.h5f.open\n",
      "OSError: Unable to open file (unable to open file: name = 'inc_cylinder_2d-75.00.pyfrs', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "!pyfr export inc_cylinder_2d.pyfrm inc_cylinder_2d-75.00.pyfrs inc_cylinder_2d-75.00.vtu -d 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python at Pawsey\n",
    "\n",
    "Pawsey has a number of solutions for Python users:\n",
    "\n",
    "- Compiled Python modules (Versions 2&3)\n",
    "- Tuned NumPy/SciPy libraries (linked agains MKL and Cray-LibSci)\n",
    "- Job-Packing Methods\n",
    "- Shifter/Singularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job-Packing\n",
    "\n",
    "Users of Magnus and Galaxy are allocated an entire node, and charged accordingly, whether they use it all or not.  Many users want to run as many single-core Python jobs on a node as possible.  The easiest way to do that is to use job-packing in your SLURM jobscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash -l\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --ntasks-per-node=24\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --partition=debugq\n",
    "#SBATCH --account=pawsey0001\n",
    "#SBATCH --export=NONE\n",
    "\n",
    "module swap PrgEnv-crady PrgEnv-gnu\n",
    "module load python\n",
    "module load numpy\n",
    "module load scipy\n",
    "module load matplotlib\n",
    "\n",
    "srun --export=ALL -n 24 -N 1 python_job_wrapper.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a single wrapper script across 24 cores.  The key is how we write our wrapper script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "python voxelSlice.py qs-curie-${SLURM_PROCID}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance of the wrapper script will call the Python interpreter, but we use the environment variable `SLURM_PROCID` to differentiate between the cores, and each core takes a different input data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit with this method is it usually require no changes to existing Python scripts, but may require some thought be given as to how to structure data inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pawsey also has Docker images available to use, particularly for Python users.  We have a program called Shifter installed on our Cray systems.  It allows for Docker containers to be run on a shared HPC system, while still maintaining performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Shifter_OSU_allgather.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/Shifter_OSU_bandwidth_reduced.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job scripts require minimal modification:\n",
    "    \n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --image=docker:pawsey/hpc-python:latest\n",
    " \n",
    " \n",
    "module load shifter\n",
    " \n",
    " \n",
    "srun -n 24 shifter python my_python_app.py <args>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Docker images provide a base of what most users would need to build their own images:\n",
    "    \n",
    "```docker\n",
    "FROM ubuntu:latest\n",
    "\n",
    "LABEL maintainer=\"brian.skjerven@pawsey.org.au\"\n",
    "\n",
    "RUN apt-get update \\\n",
    "      && apt-get install -y \\\n",
    "      cython \\\n",
    "      python-minimal \\\n",
    "      python-pip\n",
    "\n",
    "RUN pip install --upgrade pip \\\n",
    "      && pip install \\\n",
    "      astropy \\\n",
    "      h5py \\\n",
    "      matplotlib \\\n",
    "      nose \\\n",
    "      numpy \\\n",
    "      pytest \\\n",
    "      scipy \\\n",
    "      setuptools\n",
    "\n",
    "CMD [\"/bin/bash\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other benefit to using Python in a container is related to dynamic library loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/shifter_magnus.png\" style=\"height:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "- Lots of diferent ways of explointing parallelism in Python\n",
    "    * Some better sutied to different workflows\n",
    "- Make use of Pawsey compiled Python libraries (performance and module compatibility)\n",
    "- Try to use MPI capable libraries\n",
    "- Multiprocess *can* be useful, but there is a performance hit\n",
    "- Other Python options available to users (Shifter, job-packing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyfr]",
   "language": "python",
   "name": "conda-env-pyfr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
