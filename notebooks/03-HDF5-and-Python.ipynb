{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 HDF5 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Objectives:\n",
    ">\n",
    "> * Learn about HDF5 data structure\n",
    "> * Use HDF5 in Python with h5py and PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5, or Hierarchical Data Format, is data format that structures and organises data in a hierarchical way.  There are 3 components to an HDF5 file, and they function much like a directory structure on a computer does:\n",
    "\n",
    "* Groups (directories in file system) \n",
    "* Datasets (files in a file system)\n",
    "* Attributes (metadata for a file or directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/hdf5_structure.jpg\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of sequencing data stored in an HDF5 format:\n",
    "```\n",
    "                              group              name       otype  dclass       dim\n",
    "0                                 /           YAL001C   H5I_GROUP                  \n",
    "1                          /YAL001C 2016_Weinberg_RPF   H5I_GROUP                  \n",
    "2        /YAL001C/2016_Weinberg_RPF             reads   H5I_GROUP                  \n",
    "3  /YAL001C/2016_Weinberg_RPF/reads              data H5I_DATASET INTEGER 36 x 3980\n",
    "4                                 /           YAL002W   H5I_GROUP                  \n",
    "5                          /YAL002W 2016_Weinberg_RPF   H5I_GROUP                  \n",
    "6        /YAL002W/2016_Weinberg_RPF             reads   H5I_GROUP                  \n",
    "7  /YAL002W/2016_Weinberg_RPF/reads              data H5I_DATASET INTEGER 36 x 4322\n",
    "8                                 /           YAL003W   H5I_GROUP                  \n",
    "9                          /YAL003W 2016_Weinberg_RPF   H5I_GROUP                  \n",
    "10       /YAL003W/2016_Weinberg_RPF             reads   H5I_GROUP                  \n",
    "11 /YAL003W/2016_Weinberg_RPF/reads              data H5I_DATASET INTEGER 36 x 1118\n",
    "[...]\n",
    "```\n",
    "\n",
    "There are 3 sets of reads, each organised into different groups, and the data itself is arranged into an array of integers.  Each row represents a read length, and columns represent nucleotide positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cover two different Python packages for working with HDF5 data:\n",
    "\n",
    "* PyTables\n",
    "  * Provides a high-level API\n",
    "  * Adds advanced features to HDF5 like compression and improved indexing\n",
    "  * \"Batteries included\"\n",
    "* h5py\n",
    "  * Pythonic interface to HDF5\n",
    "  * Exposes the entire HDF5 library in Python\n",
    "  * Much more \"low-level\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll set up our environment for the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tables\n",
    "import h5py\n",
    "import os\n",
    "import shutil\n",
    "data_dir = \"../demos/data/test_data\"\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a an HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"layout.h5\")\n",
    "f = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/a_group (Group) ''\n",
       "  children := []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = f.create_group('/', 'a_group')\n",
    "group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets inside this group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.create_array(group, \"my_array1\", np.arange(10))\n",
    "f.create_array(group, \"my_array2\", np.ones(100).reshape(10, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/a_group/another_group (Group) ''\n",
       "  children := []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another group\n",
    "f.create_group('/a_group', 'another_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the structure of the HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../demos/data/test_data/layout.h5 (File) ''\n",
      "Last modif.: 'Fri Apr 12 00:16:58 2019'\n",
      "Object Tree: \n",
      "/ (RootGroup) ''\n",
      "/a_group (Group) ''\n",
      "/a_group/my_array1 (Array(10,)) ''\n",
      "/a_group/my_array2 (Array(10, 10)) ''\n",
      "/a_group/another_group (Group) ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural naming in PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice features of PyTables is its support for natural naming.  This is the common 'dot' notation you would have seen in object oriented programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/a_group/my_array1 (Array(10,)) ''\n",
       "  atom := Int64Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.root.a_group.my_array1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to using PyTable function calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/a_group/my_array1 (Array(10,)) ''\n",
       "  atom := Int64Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.get_node('/a_group/my_array1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(FILENAME, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_group']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With h5py, we can access objects as if they were dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/a_group\" (3 members)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['/a_group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access and view members in Python-familiar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('another_group', <HDF5 group \"/a_group/another_group\" (0 members)>),\n",
       " ('my_array1', <HDF5 dataset \"my_array1\": shape (10,), type \"<i8\">),\n",
       " ('my_array2', <HDF5 dataset \"my_array2\": shape (10, 10), type \"<f8\">)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp = f['/a_group']\n",
    "list(grp.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can then add those as datasets to our new group:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another_group', 'my_array1', 'my_array2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datatypes in HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the features of HDF5 is the ability to mix data sets and datatypes in a single file.  We'll loodk at how to do this with PyTables and h5py.  First, let's create a homogeneous data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_to_store = np.arange(10, dtype=np.int8)\n",
    "arr_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"homogenous_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several options for creating data sets; a `create_dataset` method or using a Python `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"mydata\": shape (10,), type \"|i1\">"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.create_dataset(data=arr_to_store, name=\"mydata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['/mydata2'] = arr_to_store  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mydata', 'mydata2']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the data set with the `:` we saw in NumPy earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['/mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use some utilities from the HDF5 library itself to examine datasets, `h5ls` and `h5dump`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydata                   Dataset {10}\r\n",
      "mydata2                  Dataset {10}\r\n"
     ]
    }
   ],
   "source": [
    "!h5ls {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"../demos/data/test_data/homogenous_h5py.h5\" with sec2 driver.\r\n",
      "/                        Group\r\n",
      "    Location:  1:96\r\n",
      "    Links:     1\r\n",
      "/mydata                  Dataset {10/10}\r\n",
      "    Location:  1:800\r\n",
      "    Links:     1\r\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\r\n",
      "    Type:      native signed char\r\n",
      "/mydata2                 Dataset {10/10}\r\n",
      "    Location:  1:1400\r\n",
      "    Links:     1\r\n",
      "    Storage:   10 logical bytes, 10 allocated bytes, 100.00% utilization\r\n",
      "    Type:      native signed char\r\n"
     ]
    }
   ],
   "source": [
    "!h5ls -rv {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 \"../demos/data/test_data/homogenous_h5py.h5\" {\r\n",
      "GROUP \"/\" {\r\n",
      "   DATASET \"mydata\" {\r\n",
      "      DATATYPE  H5T_STD_I8LE\r\n",
      "      DATASPACE  SIMPLE { ( 10 ) / ( 10 ) }\r\n",
      "      DATA {\r\n",
      "      (0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n",
      "      }\r\n",
      "   }\r\n",
      "   DATASET \"mydata2\" {\r\n",
      "      DATATYPE  H5T_STD_I8LE\r\n",
      "      DATASPACE  SIMPLE { ( 10 ) / ( 10 ) }\r\n",
      "      DATA {\r\n",
      "      (0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n",
      "      }\r\n",
      "   }\r\n",
      "}\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!h5dump {FILENAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "FILENAME = os.path.join(data_dir, \"homogenous_pytables.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables uses high level objects to wrap datasets:\n",
    "* **Array** - homogeneous dataset\n",
    "* **CArray** - chunked homogeneous dataset\n",
    "* **EArray** - extendable homogeneous dataset\n",
    "* **Table** - extendable, compound dataset\n",
    "\n",
    "Chuncking is an HDF5 storage layout; rather than store all of the data in a single, contiguous block in the file, data can be stored as n-dim arrays, in any order.  This can help with data access patterns and improve performance.  We won't cover chunking in this course.\n",
    "\n",
    "Extendable simply means we can use an `append()` method to add data to the file.\n",
    "\n",
    "To begin, let's create an array (homogeneous dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Array(10,)) ''\n",
       "  atom := Int8Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.create_array(f2.root, name=\"mydata\", obj=arr_to_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can read this data with either the familiar `:` notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.root.mydata[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or with a `read()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.root.mydata.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound datatypes\n",
    "\n",
    "Let's generate a data set composed of different datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = np.dtype([(\"myfield1\", np.int32), (\"myfield2\", np.float64), (\"myfield3\", \"S5\")])\n",
    "table_to_store = np.fromiter(((i, i**2, \"foo_%d\"%i) for i in range(10)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,  0., b'foo_0'), (1,  1., b'foo_1'), (2,  4., b'foo_2'),\n",
       "       (3,  9., b'foo_3'), (4, 16., b'foo_4'), (5, 25., b'foo_5'),\n",
       "       (6, 36., b'foo_6'), (7, 49., b'foo_7'), (8, 64., b'foo_8'),\n",
       "       (9, 81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_to_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_h5py.h5\")\n",
    "f = h5py.File(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['mydata'] = table_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"mydata\": shape (10,), type \"|V17\">"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,  0., b'foo_0'), (1,  1., b'foo_1'), (2,  4., b'foo_2'),\n",
       "       (3,  9., b'foo_3'), (4, 16., b'foo_4'), (5, 25., b'foo_5'),\n",
       "       (6, 36., b'foo_6'), (7, 49., b'foo_7'), (8, 64., b'foo_8'),\n",
       "       (9, 81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['mydata'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"../demos/data/test_data/compound_h5py.h5\" with sec2 driver.\r\n",
      "mydata                   Dataset {10/10}\r\n",
      "    Location:  1:800\r\n",
      "    Links:     1\r\n",
      "    Storage:   170 logical bytes, 170 allocated bytes, 100.00% utilization\r\n",
      "    Type:      struct {\r\n",
      "                   \"myfield1\"         +0    native int\r\n",
      "                   \"myfield2\"         +4    native double\r\n",
      "                   \"myfield3\"         +12   5-byte null-padded ASCII string\r\n",
      "               } 17 bytes\r\n"
     ]
    }
   ],
   "source": [
    "!h5ls -v {FILENAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTables\n",
    "\n",
    "Recall that a compound dataset is called a `Table` in PyTables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = os.path.join(data_dir, \"compound_pytables1.h5\")\n",
    "f2 = tables.open_file(FILENAME, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `create_table()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mydata (Table(0,)) ''\n",
       "  description := {\n",
       "  \"myfield1\": Int32Col(shape=(), dflt=0, pos=0),\n",
       "  \"myfield2\": Float64Col(shape=(), dflt=0.0, pos=1),\n",
       "  \"myfield3\": StringCol(itemsize=5, shape=(), dflt=b'', pos=2)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (3855,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = f2.create_table(f2.root, name=\"mydata\", description=table_to_store.dtype)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables high-level functions allows us to easily to do things like add data with `append()` or remove rows with `remove_row()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.append(table_to_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,  0., b'foo_0'), (1,  1., b'foo_1'), (2,  4., b'foo_2'),\n",
       "       (3,  9., b'foo_3'), (4, 16., b'foo_4'), (5, 25., b'foo_5'),\n",
       "       (6, 36., b'foo_6'), (7, 49., b'foo_7'), (8, 64., b'foo_8'),\n",
       "       (9, 81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.remove_row(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0,  0., b'foo_0'), (1,  1., b'foo_1'), (2,  4., b'foo_2'),\n",
       "       (3,  9., b'foo_3'), (4, 16., b'foo_4'), (6, 36., b'foo_6'),\n",
       "       (7, 49., b'foo_7'), (8, 64., b'foo_8'), (9, 81., b'foo_9')],\n",
       "      dtype=[('myfield1', '<i4'), ('myfield2', '<f8'), ('myfield3', 'S5')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is Python module that provides convenient data structures and data analysis tools.  One of the central data structures in Pandas is the DataFrame, and we can use them with HDF5 and Python.\n",
    "\n",
    "To start, let's generate a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "data_dir = \"../demos/data/hdfstore\"\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(15, 3), columns=['A', 'B', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 3 columns):\n",
      "A    15 non-null float64\n",
      "B    15 non-null float64\n",
      "C    15 non-null float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 440.0 bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.434603</td>\n",
       "      <td>0.101105</td>\n",
       "      <td>-1.054922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.301304</td>\n",
       "      <td>-2.718253</td>\n",
       "      <td>0.280248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.672155</td>\n",
       "      <td>-0.477084</td>\n",
       "      <td>-0.159161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068657</td>\n",
       "      <td>-1.278624</td>\n",
       "      <td>-0.553689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.661793</td>\n",
       "      <td>0.610631</td>\n",
       "      <td>1.607877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "0  0.434603  0.101105 -1.054922\n",
       "1  0.301304 -2.718253  0.280248\n",
       "2  0.672155 -0.477084 -0.159161\n",
       "3  0.068657 -1.278624 -0.553689\n",
       "4 -1.661793  0.610631  1.607877"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15239934298136623"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['A'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily store a Panda dataframe as an HDF5 file with the `HDFStore()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HDF5 file to store the \n",
    "fn = os.path.join(data_dir, 'test.h5')\n",
    "hdfstore = pd.HDFStore(fn, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas.HDFStore acts like a dict similar to h5py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfstore['my_array'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: ../demos/data/hdfstore/test.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hdfstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.io.pytables.HDFStore'>\n",
       "File path: ../demos/data/hdfstore/test.h5"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfstore.put('my_table', df[:5], format='table')\n",
    "hdfstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfstore.put('my_table', df[:5], format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfstore.append('my_table', df[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfstore.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\r\n",
      "/my_array (Group) ''\r\n",
      "/my_array/axis0 (Array(3,)) ''\r\n",
      "  atom := StringAtom(itemsize=1, shape=(), dflt=b'')\r\n",
      "  maindim := 0\r\n",
      "  flavor := 'numpy'\r\n",
      "  byteorder := 'irrelevant'\r\n",
      "  chunkshape := None\r\n",
      "/my_array/axis1 (Array(15,)) ''\r\n",
      "  atom := Int64Atom(shape=(), dflt=0)\r\n",
      "  maindim := 0\r\n",
      "  flavor := 'numpy'\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := None\r\n",
      "/my_array/block0_items (Array(3,)) ''\r\n",
      "  atom := StringAtom(itemsize=1, shape=(), dflt=b'')\r\n",
      "  maindim := 0\r\n",
      "  flavor := 'numpy'\r\n",
      "  byteorder := 'irrelevant'\r\n",
      "  chunkshape := None\r\n",
      "/my_array/block0_values (Array(15, 3)) ''\r\n",
      "  atom := Float64Atom(shape=(), dflt=0.0)\r\n",
      "  maindim := 0\r\n",
      "  flavor := 'numpy'\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := None\r\n",
      "/my_table (Group) ''\r\n",
      "/my_table/table (Table(15,)) ''\r\n",
      "  description := {\r\n",
      "  \"index\": Int64Col(shape=(), dflt=0, pos=0),\r\n",
      "  \"values_block_0\": Float64Col(shape=(3,), dflt=0.0, pos=1)}\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := (2048,)\r\n",
      "  autoindex := True\r\n",
      "  colindexes := {\r\n",
      "    \"index\": Index(6, medium, shuffle, zlib(1)).is_csi=False}\r\n"
     ]
    }
   ],
   "source": [
    "!ptdump -v {fn}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF\n",
    "\n",
    "In addition to HDF5, another common I/O library is NetCDF.  NetCDF is built upon HDF5 (newer versions), and has a similar structure (datasets have dimensions, variables, and attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4_CLASSIC data model, file format HDF5):\n",
      "    Conventions: CF-1.0\n",
      "    title: HYCOM ATLb2.00\n",
      "    institution: National Centers for Environmental Prediction\n",
      "    source: HYCOM archive file\n",
      "    experiment: 90.9\n",
      "    history: archv2ncdf3z\n",
      "    dimensions(sizes): MT(1), Y(850), X(712), Depth(10)\n",
      "    variables(dimensions): float64 \u001b[4mMT\u001b[0m(MT), float64 \u001b[4mDate\u001b[0m(MT), float32 \u001b[4mDepth\u001b[0m(Depth), int32 \u001b[4mY\u001b[0m(Y), int32 \u001b[4mX\u001b[0m(X), float32 \u001b[4mLatitude\u001b[0m(Y,X), float32 \u001b[4mLongitude\u001b[0m(Y,X), float32 \u001b[4mu\u001b[0m(MT,Depth,Y,X), float32 \u001b[4mv\u001b[0m(MT,Depth,Y,X), float32 \u001b[4mtemperature\u001b[0m(MT,Depth,Y,X), float32 \u001b[4msalinity\u001b[0m(MT,Depth,Y,X)\n",
      "    groups: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = netCDF4.Dataset('../demos/data/rtofs_glo_3dz_f006_6hrly_reg3.nc')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's look at the variables in a netCDF file.  We can access them via a Python `dict`, much like we did with HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['MT', 'Date', 'Depth', 'Y', 'X', 'Latitude', 'Longitude', 'u', 'v', 'temperature', 'salinity'])\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 temperature(MT, Depth, Y, X)\n",
      "    coordinates: Longitude Latitude Date\n",
      "    standard_name: sea_water_potential_temperature\n",
      "    units: degC\n",
      "    _FillValue: 1.2676506e+30\n",
      "    valid_range: [-5.078603  11.1498995]\n",
      "    long_name:   temp [90.9H]\n",
      "unlimited dimensions: MT\n",
      "current shape = (1, 10, 850, 712)\n",
      "filling on\n"
     ]
    }
   ],
   "source": [
    "print(f.variables.keys()) # get all variable names\n",
    "temp = f.variables['temperature']  # temperature variable\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables in a netCDF file have dimensions associated with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MT', <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'MT', size = 1\n",
      ")\n",
      "('Y', <class 'netCDF4._netCDF4.Dimension'>: name = 'Y', size = 850\n",
      ")\n",
      "('X', <class 'netCDF4._netCDF4.Dimension'>: name = 'X', size = 712\n",
      ")\n",
      "('Depth', <class 'netCDF4._netCDF4.Dimension'>: name = 'Depth', size = 10\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for d in f.dimensions.items():\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each variable has attributes `temp` and `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MT', 'Depth', 'Y', 'X')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 850, 712)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access data in netCDF files much like we do with NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float64 MT(MT)\n",
      "    long_name: time\n",
      "    units: days since 1900-12-31 00:00:00\n",
      "    calendar: standard\n",
      "    axis: T\n",
      "unlimited dimensions: MT\n",
      "current shape = (1,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "int32 X(X)\n",
      "    point_spacing: even\n",
      "    axis: X\n",
      "unlimited dimensions: \n",
      "current shape = (712,)\n",
      "filling on, default _FillValue of -2147483647 used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mt = f.variables['MT']\n",
    "depth = f.variables['Depth']\n",
    "x,y = f.variables['X'], f.variables['Y']\n",
    "print(mt)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41023.25]\n"
     ]
    }
   ],
   "source": [
    "time = mt[:]  # Reads the netCDF variable MT, array of one element\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "* Binary, parallel file formats like HDF5 and netCDF are vital to performance on HPC systems\n",
    "* Python has convenient modules to access this (don't need to write a C library to read in your data)\n",
    "* The libraries also have parallel version (read/write across 1000s of processors\n",
    "* Consider adapting your data to HDF5...it's more flexible and performant in the long run\n",
    "* Still allows you to work with things like Pandas and Matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
